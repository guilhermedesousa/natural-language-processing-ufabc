{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6QILOdpOjwv"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2023.Q3]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m67OOx9MX_3"
      },
      "source": [
        "### **ATIVIDADE PRÁTICA 04 [Uso da API da OpenAI com técnicas de PLN]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gk0nHKabBT-"
      },
      "source": [
        "A **ATIVIDADE PRÁTICA 04** deve ser feita utilizando o **Google Colab** com uma conta sua vinculada ao Gmail. O link do seu notebook, armazenado no Google Drive, além do link de um repositório no GitHub e os principais resultados da atividade, devem ser enviados usando o seguinte formulário:\n",
        "\n",
        "> https://forms.gle/GzwCq3R7ExtE9g9a8\n",
        "\n",
        "\n",
        "**IMPORTANTE**: A submissão deve ser feita até o dia **26/11 (domingo)** APENAS POR UM INTEGRANTE DA EQUIPE, até às 23h59. Por favor, lembre-se de dar permissão de ACESSO IRRESTRITO para o professor da disciplina de PLN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7hJlilKM485"
      },
      "source": [
        "### **EQUIPE**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POR FAVOR, PREENCHER OS INTEGRANDES DA SUA EQUIPE:**\n",
        "\n",
        "\n",
        "**Integrante 01:**\n",
        "\n",
        "Carlos Henrique Alencar Lima - 11202021040\n",
        "\n",
        "**Integrante 02:**\n",
        "\n",
        "Guilherme de Sousa Santos - 11201921175\n",
        "\n",
        "**Integrante 03:**\n",
        "\n",
        "José Roberto de Oliveira - 11201920397"
      ],
      "metadata": {
        "id": "tnIArN0QY-Ek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LIVRO**\n",
        "---"
      ],
      "metadata": {
        "id": "6yExhaebs-nD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Processamento de Linguagem Natural - Conceitos, Técnicas e Aplicações em Português.`\n",
        "\n",
        ">\n",
        "\n",
        "Disponível gratuitamente em:\n",
        "  \n",
        "  > https://brasileiraspln.com/livro-pln/1a-edicao/.\n",
        "\n",
        "\n",
        "**POR FAVOR, PREENCHER OS CAPITULOS SELECIONADOS PARA A SUA EQUIPE:**\n",
        "\n",
        "`Primeiro capítulo: ` 4 - Sequência de caracteres e palavras\n",
        "\n",
        "`Segundo capítulo:` 22 - PLN no Direito\n",
        "\n"
      ],
      "metadata": {
        "id": "DjJM_qhEZRy6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtjgWQRzNphL"
      },
      "source": [
        "### **DESCRIÇÃO**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementar um `notebook` no `Google Colab` que faça uso da **API da OpenAI** aplicando, no mínimo, 3 técnicas de PLN. As técnicas devem ser aplicadas nos 2 (DOIS) capítulos do livro **Processamento de Linguagem Natural - Conceitos, Técnicas e Aplicações em Português**.\n",
        "\n",
        ">\n",
        "\n",
        "**RESTRIÇÃO**: É obrigatório usar o *endpoint* \"*`Chat Completions`*\".\n",
        "\n",
        ">\n",
        "\n",
        "As seguintes técnicas de PLN podem ser usadas:\n",
        "\n",
        "*   Correção Gramatical\n",
        "*   Classificação de Textos\n",
        "*   Análise de Sentimentos\n",
        "*   Detecção de Emoções\n",
        "*   Extração de Palavras-chave\n",
        "*   Tradução de Textos\n",
        "*   Sumarização de Textos\n",
        "*   **Similaridade de Textos**\n",
        "*   **Reconhecimento de Entidades Nomeadas**\n",
        "*   **Sistemas de Perguntas e Respostas**\n",
        "\n",
        ">\n",
        "\n",
        "Os capítulos devem ser os mesmos selecionados na **ATIVIDADE PRÁTICA 02**. Para consultar os capítulos, considere a seguinte planilha:\n",
        "\n",
        ">\n",
        "\n",
        "> https://docs.google.com/spreadsheets/d/1ZutzQ3v1OJgsgzCvCwxXlRIQ3ChXNlHNvB63JQvYsbo/edit?usp=sharing\n",
        "\n",
        ">\n",
        ">\n",
        "\n",
        "**IMPORTANTE:** É obrigatório usar o e-mail da UFABC. Não é permitido alterar os capítulos já selecionados.\n",
        "\n"
      ],
      "metadata": {
        "id": "fXTwkiiGs2BV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CRITÉRIOS DE AVALIAÇÃO**\n",
        "---\n"
      ],
      "metadata": {
        "id": "gWsBYQNtxmum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Serão considerados como critérios de avaliação as técnicas usadas e a criatividade envolvida na aplicação das mesmas.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5iHdx4BXYruQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **IMPLEMENTAÇÃO**\n",
        "---"
      ],
      "metadata": {
        "id": "nw09lujGvfjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instalação da biblioteca da API da OpenAI**"
      ],
      "metadata": {
        "id": "awnUz82h7FNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Instalando a biblioteca da API da OpenAi...\")\n",
        "\n",
        "!pip install openai==0.28.1\n",
        "\n",
        "print(\"API da OpenAI instalada!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyWep7xe7Ddp",
        "outputId": "d50db0d9-e92b-47f5-defd-028266eac2c8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instalando a biblioteca da API da OpenAi...\n",
            "Requirement already satisfied: openai==0.28.1 in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.3.1)\n",
            "API da OpenAI instalada!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importando as bibliotecas necessárias**"
      ],
      "metadata": {
        "id": "8_NE0zaG5biZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import requests\n",
        "import re\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from pydantic import BaseModel\n",
        "from tenacity import retry, wait_exponential, stop_after_attempt\n",
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk import download\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "E9ZtriseNIEG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d860768-e74e-4fab-fe97-bfb589415218"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V74zgTmUqHM1",
        "outputId": "42f588cf-e7fd-4d3d-e681-98c44dd46cc3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Versão da API da OpenAI**"
      ],
      "metadata": {
        "id": "y9ZY0UD259Mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(openai.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCg5VaC05bKA",
        "outputId": "86f4bdc3-47ac-491e-abfc-0593cc0459db"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definindo algumas constantes e variáveis**"
      ],
      "metadata": {
        "id": "UlXBMwsA6KHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# definir a chave da API\n",
        "API_KEY = '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'\n",
        "openai.api_key = API_KEY\n",
        "\n",
        "# definir o modelo da OpenAI\n",
        "MODEL = \"gpt-3.5-turbo\"\n",
        "\n",
        "# URLs dos capítulos 4 e 22\n",
        "chapters = [\n",
        "    {\n",
        "        \"name\": \"Capítulo 4: Sequência de caracteres e palavras\",\n",
        "        \"url\": \"https://brasileiraspln.com/livro-pln/1a-edicao/parte3/cap4/cap4.html\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Capítulo 22: PLN no Direito\",\n",
        "        \"url\": \"https://brasileiraspln.com/livro-pln/1a-edicao/parte9/cap22/cap22.html\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "cDr8ISRa6SzF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definindo algumas funções auxiliares**"
      ],
      "metadata": {
        "id": "RZmYa_Hw9nA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_from_paragraphs(paragraphs):\n",
        "  text = \"\"\n",
        "\n",
        "  for p in paragraphs:\n",
        "    text += p.getText() + \"\\n\"\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "uoGbHnnqH60n"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    characters_to_reduce = ['!', '@', '#']\n",
        "\n",
        "    for char in characters_to_reduce:\n",
        "        pattern = re.escape(char) + \"{2,}\"\n",
        "        text = re.sub(pattern, char, text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "6R994Fuyjy9b"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text(text, max_tokens=1000):\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "\n",
        "  chunks = []\n",
        "  current_chunk = []\n",
        "  current_chunk_tokens = 0\n",
        "\n",
        "  for token in tokens:\n",
        "    current_chunk.append(token)\n",
        "    current_chunk_tokens += 1\n",
        "\n",
        "    if (current_chunk_tokens >= max_tokens):\n",
        "      chunks.append(' '.join(current_chunk))\n",
        "      current_chunk = []\n",
        "      current_chunk_tokens = 0\n",
        "\n",
        "  if (current_chunk_tokens > 0):\n",
        "    chunks.append(' '.join(current_chunk))\n",
        "\n",
        "  return chunks"
      ],
      "metadata": {
        "id": "sM_Y50xUo-yE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text(text):\n",
        "    result = {\"text\": text}\n",
        "    example = []\n",
        "\n",
        "    @retry(stop=stop_after_attempt(5), wait=wait_exponential(multiplier=0.5, max=5))\n",
        "\n",
        "    # usa o endpoint chat completion da OpenAI\n",
        "    def call_openai_chat_completion(messages, functions, function_call):\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=MODEL,\n",
        "                messages=messages,\n",
        "                functions=functions,\n",
        "                function_call=function_call,\n",
        "            )\n",
        "\n",
        "            function_call_response = response.choices[0].message[\"function_call\"]\n",
        "            argument = json.loads(function_call_response[\"arguments\"])\n",
        "\n",
        "            return argument\n",
        "        except openai.error.TimeoutError as e:\n",
        "            raise e\n",
        "\n",
        "    sentiment_count = {\"positive\": 0, \"negative\": 0, \"neutral\": 0, \"mixed\": 0}\n",
        "    final_topics_count = {\"positive\": {}, \"negative\": {}, \"neutral\": {}, \"mixed\": {}}\n",
        "\n",
        "    '''\n",
        "    Função para resumir o texto (text summarize)\n",
        "    '''\n",
        "\n",
        "    functions_summary = {\n",
        "        \"name\": \"print_summary\",\n",
        "        \"description\": \"A function that prints the given summary\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"summary\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The summarized text\",\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"summary\"]\n",
        "        },\n",
        "    }\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": text}]\n",
        "\n",
        "    function_call = {\n",
        "        \"name\": functions_summary[\"name\"],\n",
        "        \"arguments\": json.dumps({\n",
        "            \"summary\": text,\n",
        "            \"max_tokens\": 500\n",
        "        })\n",
        "    }\n",
        "\n",
        "    text = call_openai_chat_completion(messages, [functions_summary], function_call)['summary']\n",
        "\n",
        "    result[\"summarized_text\"] = text\n",
        "\n",
        "    preprocessed_text = preprocess_text(text)  # Correctly calling the function here fazendo qa\n",
        "    sentences = sent_tokenize(preprocessed_text)\n",
        "\n",
        "    '''\n",
        "    Função de analise de sentimento\n",
        "    '''\n",
        "\n",
        "    functions_sentiment = [{\n",
        "        \"name\": \"print_sentiment\",\n",
        "        \"description\": \"A function that prints the given sentiment\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"sentiment\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"enum\": [\"positive\", \"negative\", \"neutral\", \"mixed\"],\n",
        "                    \"description\": \"The sentiment\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"sentiment\"]\n",
        "        }\n",
        "    }]\n",
        "\n",
        "    '''\n",
        "    Função de extração de palavras-chaves\n",
        "    '''\n",
        "\n",
        "    functions_keywords = [{\n",
        "        \"name\": \"print_keywords\",\n",
        "        \"description\": \"A function that prints the given keywords\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"keywords\": {\n",
        "                    \"type\": \"array\",\n",
        "                    \"items\": {\n",
        "                        \"type\": \"string\"\n",
        "                    }\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"keywords\"]\n",
        "        }\n",
        "    }]\n",
        "\n",
        "    functions = functions_sentiment + functions_keywords\n",
        "    sentence_results = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        messages = [{\"role\": \"user\", \"content\": sentence}]\n",
        "        sentence_result = {\"sentence\": sentence}\n",
        "\n",
        "        for function in functions:\n",
        "            function_call = {\"name\": function[\"name\"]}\n",
        "            if function[\"name\"] == \"print_sentiment\":\n",
        "                function_call[\"arguments\"] = json.dumps({\n",
        "                    \"sentiment\": [\"positive\", \"negative\", \"neutral\", \"mixed\"]\n",
        "                })\n",
        "\n",
        "            elif function[\"name\"] == \"print_keywords\":\n",
        "                function_call[\"arguments\"] = json.dumps({\n",
        "                    \"keywords\": sentence\n",
        "                })\n",
        "\n",
        "            argument = call_openai_chat_completion(messages, functions, function_call)\n",
        "            sentence_result[function[\"name\"]] = argument\n",
        "\n",
        "            if function[\"name\"] == \"print_sentiment\":\n",
        "                sentiment = argument[\"sentiment\"]\n",
        "                sentiment_count[sentiment] += 1\n",
        "\n",
        "        sentiment = sentence_result.get(\"print_sentiment\", {}).get(\"sentiment\", \"neutral\")\n",
        "\n",
        "        sentence_results.append(sentence_result)\n",
        "\n",
        "    total_sentiments = sum(sentiment_count.values())\n",
        "    percent_positive = (sentiment_count[\"positive\"] / total_sentiments) * 100\n",
        "    percent_negative = (sentiment_count[\"negative\"] / total_sentiments) * 100\n",
        "    SSCORE = percent_positive - percent_negative\n",
        "\n",
        "    result.update({\"sentence_results\": sentence_results, \"final_sentiment_count\": sentiment_count, \"SSCORE\": SSCORE,})\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "MYhbVrnhBLCx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testando**"
      ],
      "metadata": {
        "id": "GgbiITCAg8M5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_chapters = {}\n",
        "\n",
        "for chapter in chapters:\n",
        "  processed_paragraphs = []\n",
        "\n",
        "  name = chapter[\"name\"]\n",
        "  url = chapter[\"url\"]\n",
        "\n",
        "  print(name)\n",
        "\n",
        "  response = requests.get(url)\n",
        "  soup = BeautifulSoup(response.content, 'html.parser')\n",
        "  book_text_container = soup.find('main')\n",
        "  paragraphs = book_text_container.find_all(['p'])\n",
        "  text = get_text_from_paragraphs(paragraphs)\n",
        "\n",
        "  chunks = split_text(text)\n",
        "\n",
        "  for chunk in tqdm(chunks):\n",
        "    result = process_text(chunk)\n",
        "    processed_paragraphs.append(result)\n",
        "\n",
        "  processed_chapters[name] = processed_paragraphs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLU27-M6FO9r",
        "outputId": "ef123dfc-df04-4513-d744-212562ec9970"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Capítulo 4: Sequência de caracteres e palavras\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [24:29<00:00, 113.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Capítulo 22: PLN no Direito\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [07:35<00:00, 65.03s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Salvar o resultado no arquivo data.json"
      ],
      "metadata": {
        "id": "1UjsroP1k-C2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('data.json', 'w', encoding=\"utf-8\") as f:\n",
        "    json.dump(processed_chapters, f, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "Kzu29ngCxkvG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resultados"
      ],
      "metadata": {
        "id": "zvSwVXdFofRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('data.json', 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)"
      ],
      "metadata": {
        "id": "QWImc5FgpNqK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEqzvhQ9p2uO",
        "outputId": "5c8445e8-c44c-480e-d30d-57e86e5b76a5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Capítulo 4: Sequência de caracteres e palavras': [{'text': 'Morfologia e morfossintaxe Maria José Bocorny Finatto Helena de Medeiros Caseli Lucelene Lopes Amanda Rassi 26/09/2023 PDF Neste capítulo , trataremos de algo que parece simples , mas não é : identificar a unidade mínima quando tratamos , computacionalmente , a língua . Essa delimitação não é consensual entre pesquisadores e profissionais de PLN . E , mesmo em linguística , há sempre controvérsias e necessidade de pontos de referência para se definir , por exemplo , o que seja uma palavra ou mesmo uma frase . As subáreas especializadas dos estudos linguísticos entendem como unidade mínima de processamento diferentes elementos conforme seus focos e pontos de vista . A fonologia ( Capítulo 2 ) , por exemplo , considera o fonema como a menor unidade sonora e distintiva de uma língua . Se tomarmos o exemplo do que diferencia as palavras “ sábia ” ( mulher inteligente ) , “ sabiá ” ( pássaro ) e “ sabia ” ( verbo “ saber ” ) , percebemos que a sílaba tônica é o diferencial , especialmente quando pensamos em sons e fala e não em escrita . Já a morfologia considera o morfema como a menor unidade dotada de significado na língua . Nessa perspectiva , temos os “ pedacinhos ” de palavras e seus valores , como seria o caso da marca de diminutivo “ -inho ” , que assinala o masculino e o singular em “ menininho ” , ou o segmento “ -ei ” no verbo “ comprei ” , que marca um modo-tempo ( pretérito perfeito do modo indicativo ) e também um número-pessoa ( primeira pessoa do singular ) . Assim , conforme o ponto de vista de quem analisa , uma palavra pode ser feita de sons e de sílabas tônicas e/ou composta de vários segmentos gráficos menores . Podemos , ainda , considerar segmentos ou pedaços mais abrangentes , conforme o critério que utilizamos . Um exemplo nessa linha seria a palavra “ guarda-pó ” , que pode ser considerada como uma palavra só ou a junção de duas palavras . Outro caso ilustrativo é “ escova de dente ” , que , para alguns , é a união de três palavras , e , para outros , é uma palavra só , mesmo que não tenha hífen . Além dessas questões , também é controverso tratar das abreviaturas , siglas , interjeições , dos modos de escrita diferenciados nas redes sociais , com internetês , hashtags , emojis , símbolos e outras peculiaridades . Fazendo um paralelo , podemos entender que , de modo geral , os modelos de PLN trabalham as palavras como unidade primária de processamento . Vejamos , por exemplo , o caso da frase no Exemplo 4.1 . Exemplo 4.1 Jacinta Maria comprou uma cadeira em São Paulo ontem e pagou 25 reais por ela . Na frase do Exemplo 4.1 são 15 palavras se considerarmos que é palavra toda a sequência de caracteres separada por um espaço em branco . Mas se pode pensar que Jacinta+Maria e São+Paulo são palavras compostas e que , talvez , o número 25 não seja bem uma palavra , não ? A resposta será : depende do critério que você usar e da finalidade que tem ou busca com essa referência de unidade e/ou partes . Ao fazer o processamento computacional de textos escritos , a definição de que tipo de unidade de processamento se quer buscar/estudar parece estar atrelada às necessidades da tarefa ou trabalho pretendidos . Geralmente , considera-se que uma palavra é , simplesmente , uma unidade grafológica delimitada , nas línguas europeias , entre espaços em branco na representação gráfica , ou entre um espaço em branco e um sinal de pontuação1 . Essa é uma definição bastante concreta , e bastante prática . No entanto , ao pensarmos em nossos modelos computacionais e suas aplicações no mundo , é importante nos aprofundarmos um pouco mais na conceituação do que é uma palavra e nas possibilidades de processamento e implicações das decisões tomadas no pré-processamento dos corpora . Segundo Cabré ( 1999 , p. 20 ) , as palavras são as unidades de referência da realidade empregadas pelos falantes . De acordo com essa definição , as palavras compõem a dimensão linguística mais estreitamente ligada ao mundo real . Ainda segundo a mesma autora , o léxico consiste no conjunto das palavras de uma língua e dos padrões que possibilitam a criatividade do falante . As palavras e , principalmente , as associações infinitas e imprevisíveis que os seres humanos são capazes de traçar entre elas , constituem a manifestação mais concreta e mais produtiva da língua . Assim , é importante que , ao processar dados textuais para gerar modelos computacionais , nos recordemos sempre de que não estamos simplesmente organizando um conjunto de caracteres ou ordenando uma representação ortográfica formal , mas que estamos trabalhando com recursos linguísticos que representam a experiência humana . Em seguida , devemos considerar também o conceito de palavra computacional , que se refere a uma unidade linguística que foi adaptada ou criada especificamente para facilitar seu processamento por máquinas . Isso pode envolver a manipulação de palavras , frases ou até mesmo caracteres de maneira que seja mais conveniente para algoritmos e sistemas de PLN lidarem com elas . A necessidade dessas palavras computacionais surge devido às complexidades do processamento de linguagem natural por máquinas . A linguagem humana é rica e ambígua , cheia de nuances e variações que podem ser difíceis de interpretar e analisar automaticamente . Portanto , ao transformar palavras em formas mais padronizadas ou simplificadas , os sistemas de PLN podem executar tarefas como análise gramatical , extração de informações ( Capítulo 17 ) e tradução ( Capítulo 18 ) com maior eficácia . Dependendo do objetivo da tarefa ou aplicação de PLN , é possível definir quais rotinas de pré-processamento são mais produtivas para criar as palavras computacionais , ou seja ,', 'summarized_text': 'Neste capítulo, são abordadas questões relacionadas à identificação da unidade mínima na linguagem, tanto no campo computacional quanto na linguística. A delimitação dessa unidade não é consensual, variando de acordo com o foco e ponto de vista dos pesquisadores. A fonologia considera o fonema como a menor unidade sonora, enquanto a morfologia considera o morfema como a menor unidade dotada de significado. Além disso, há controvérsias sobre o que é considerado uma palavra, seja pela sua composição sonora, escrita ou mesmo pela combinação de palavras. Também são discutidas as questões relacionadas a abreviaturas, siglas, interjeições e outros elementos linguísticos presentes nas redes sociais. No campo do processamento de linguagem natural, a definição da unidade de processamento depende das necessidades da tarefa ou trabalho em questão. Por fim, também é abordado o conceito de palavra computacional, adaptada ou criada para facilitar o processamento por máquinas, levando em consideração a complexidade e ambiguidade da linguagem humana.', 'sentence_results': [{'sentence': 'Neste capítulo, são abordadas questões relacionadas à identificação da unidade mínima na linguagem, tanto no campo computacional quanto na linguística.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['identificação', 'unidade mínima', 'linguagem', 'computacional', 'linguística']}}, {'sentence': 'A delimitação dessa unidade não é consensual, variando de acordo com o foco e ponto de vista dos pesquisadores.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['delimitação', 'unidade', 'consensual', 'variando', 'foco', 'ponto de vista', 'pesquisadores']}}, {'sentence': 'A fonologia considera o fonema como a menor unidade sonora, enquanto a morfologia considera o morfema como a menor unidade dotada de significado.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['fonologia', 'fonema', 'morfologia', 'morfema']}}, {'sentence': 'Além disso, há controvérsias sobre o que é considerado uma palavra, seja pela sua composição sonora, escrita ou mesmo pela combinação de palavras.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['composição sonora', 'composição escrita', 'combinação de palavras']}}, {'sentence': 'Também são discutidas as questões relacionadas a abreviaturas, siglas, interjeições e outros elementos linguísticos presentes nas redes sociais.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['abreviaturas', 'siglas', 'interjeições', 'redes sociais']}}, {'sentence': 'No campo do processamento de linguagem natural, a definição da unidade de processamento depende das necessidades da tarefa ou trabalho em questão.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['processamento de linguagem natural', 'unidade de processamento', 'tarefa', 'trabalho']}}, {'sentence': 'Por fim, também é abordado o conceito de palavra computacional, adaptada ou criada para facilitar o processamento por máquinas, levando em consideração a complexidade e ambiguidade da linguagem humana.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['palavra computacional', 'processamento por máquinas', 'complexidade da linguagem humana', 'ambiguidade da linguagem humana']}}], 'final_sentiment_count': {'positive': 0, 'negative': 0, 'neutral': 7, 'mixed': 0}, 'SSCORE': 0.0}, {'text': 'pode-se remover ou não acentos ortográficos , espaços em branco em itens como “ fim de semana ” , ou hífens como em “ guarda-chuva ” . No mesmo sentido , dependendo dos propósitos da tarefa , um modelo de aprendizado de máquina pode precisar definir expressões multipalavras ( Capítulo 5 ) ( e.g . “ nem que a vaca tussa ” , “ deus me livre ” ou “ ciência de dados ” ) , hashtags ( e.g . “ # euamopuzzles ” ) , URLs e outros compostos como palavras ( ou unidades lexicais ) únicas , como se fossem representadas sem espaços : “ nemqueavacatussa ” , “ deusmelivre ” , “ ciênciadedados ” e “ euamopuzzles ” . Essas e outras estratégias similares podem facilitar o processamento automático , mas devem considerar as necessidades de cada aplicação computacional . Este capítulo apresenta conceitos ( Seção 4.1 ) relacionados a essas unidades mínimas de processamento e que devem ser considerados quando lidamos com textos . Em seguida , são apresentadas as tarefas relacionadas ao processamento morfológico dos textos ( Seção 4.2 ) . E depois são indicadas as ferramentas e os recursos disponíveis para o português , exemplificando com uma aplicação prática de um recurso específico para o português brasileiro ( Seção 4.3 ) . Por fim , fazemos uma retomada dos principais tópicos discutidos , apresentando as considerações finais e os planos para uma futura versão deste capítulo ( Seção 4.4 ) . Antes de vermos como identificar e tratar computacionalmente as unidades mínimas de processamento , cabe definir alguns conceitos linguísticos básicos necessários . Nesta seção , definimos os conceitos de Morfema ( e todos os seus tipos ) ( Seção 4.1.1 ) , Token e Type ( Seção 4.1.2 ) , Lexema , Lexia e Lema ( Seção 4.1.3 ) , Léxico e Gramática ( Seção 4.1.4 ) , Léxico comum e especializado ( Seção 4.1.5 ) , além de palavras funcionais e lexicais ( Seção 4.1.6 ) . Essa seção traz , ainda , informações sobre os processos de formação das palavras ( Seção 4.1.7 ) e morfologia e morfossintaxe ( Seção 4.1.8 ) . O objeto principal de estudo da morfologia é o morfema , definido linguisticamente como “ unidade mínima significativa ” . Segundo essa definição , o morfema é a menor unidade linguística dotada de significado , considerando que há outras unidades linguísticas que também possuem significado , como a palavra , o sintagma , a frase , a oração , o período , o texto etc . Além disso , o morfema é considerado como “ dotado de significado ” por oposição ao fonema , que é o objeto de estudo da fonética e da fonologia , e , na verdade , é a menor unidade de análise linguística , porém não possui significado em si , mas tem a função de estabelecer diferença de significado entre uma palavra outra . Em outras palavras , o fonema é apenas uma unidade linguística distintiva ( não significativa ) , pois diferencia as palavras por meio de seus sons ( e.g . “ faca ” e “ vaca ” são duas palavras diferentes que se distinguem pelo fonema inicial “ \\\\f ” ou “ \\\\v ” ) . O mesmo vale para o trio “ sábia ” , “ sabia ” e “ sabiá ” , lembrando que estamos no território dos sons e não da escrita . Para saber mais sobre o processamento da fala , sugere-se a leitura de Capítulo 2 e Capítulo 3 . Em uma explicação simples , podemos dizer que os morfemas são os pedacinhos que se juntam para formar as palavras . E esses “ pedacinhos ” podem ser de vários tipos : desinência , raiz , radical , afixo , vogal temática e tema . Por exemplo , podemos juntar o radical “ experiment ” com a vogal temática “ a ” com a desinência verbal “ ri ” e com a desinência verbal “ a ” para formar a palavra “ experimentaria ” . Esses quatro “ pedacinhos ” são chamados de morfemas e eles possuem significados : A seguir apresentamos brevemente uma definição e exemplos dos vários tipos de morfemas em português . Desinências são os morfemas que geralmente ficam no final da palavra e podem marcar gênero e número ( no caso dos substantivos e adjetivos ) ou marcar número , pessoa , tempo e modo ( no caso dos verbos ) . Por isso as desinências podem ser classificadas em : nominais ou verbais . Em português , o substantivo “ meninas ” é formado pelo radical “ menin ” + duas desinências nominais : “ a ” , que indica feminino , e “ s ” , que indica plural . Assim , dizemos que a palavra “ meninas ” está flexionada no feminino plural . Já o verbo “ adotássemos ” é formado pelo radical “ adot ” + a vogal temática “ a ” + duas desinências verbais : “ sse ” , que indica modo e tempo ( pretérito imperfeito do subjuntivo ) , e “ mos ” , que indica número e pessoa ( primeira pessoa do plural ) . Assim , dizemos que o verbo “ adotássemos ” está flexionado na primeira pessoa do plural do pretérito imperfeito do subjuntivo . Na linguística teórica , existe uma diferença conceitual entre raiz e radical . Embora a definição de ambos os conceitos ressalte que são os constituintes da palavra que contêm significado lexical , no caso da raiz , ela não inclui afixos derivacionais ou flexionais ( e.g . “ beb ” é a raiz de “ beber ” , “ beberemos ” , “ bebendo ” , “ bebida ” , “ bebidinhas ” e tantas outras formas flexionadas ) . No caso do radical , ele não inclui afixos de flexão , mas pode incluir afixos derivacionais ( e.g . “ beb ”', 'summarized_text': 'Neste texto, é discutido o uso de acentos ortográficos, espaços em branco e hífens em palavras. Além disso, são apresentadas estratégias para lidar com expressões multipalavras, hashtags, URLs e outros compostos como palavras únicas. Também são definidos conceitos linguísticos básicos relacionados ao processamento de texto. O texto aborda ainda as tarefas relacionadas ao processamento morfológico e indica as ferramentas e recursos disponíveis para o português. Por fim, é feita uma retomada dos principais tópicos discutidos e são apresentadas considerações finais e planos para futuras versões. São explicados os conceitos de morfema, token, type, lexema, lexia, lema, léxico e gramática. São fornecidas definições e exemplos de diferentes tipos de morfemas em português, como desinências, raiz, radical, afixo, vogal temática e tema.', 'sentence_results': [{'sentence': 'Neste texto, é discutido o uso de acentos ortográficos, espaços em branco e hífens em palavras.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['acentos ortográficos', 'espaços em branco', 'hífens']}}, {'sentence': 'Além disso, são apresentadas estratégias para lidar com expressões multipalavras, hashtags, URLs e outros compostos como palavras únicas.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['expressões multipalavras', 'hashtags', 'URLs', 'palavras únicas']}}, {'sentence': 'Também são definidos conceitos linguísticos básicos relacionados ao processamento de texto.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['processamento de texto', 'linguística', 'conceitos básicos']}}, {'sentence': 'O texto aborda ainda as tarefas relacionadas ao processamento morfológico e indica as ferramentas e recursos disponíveis para o português.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['processamento morfológico', 'ferramentas', 'recursos', 'português']}}, {'sentence': 'Por fim, é feita uma retomada dos principais tópicos discutidos e são apresentadas considerações finais e planos para futuras versões.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['retomada', 'principais tópicos', 'considerações finais', 'planos para futuras versões']}}, {'sentence': 'São explicados os conceitos de morfema, token, type, lexema, lexia, lema, léxico e gramática.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['morfema', 'token', 'type', 'lexema', 'lexia', 'lema', 'léxico', 'gramática']}}, {'sentence': 'São fornecidas definições e exemplos de diferentes tipos de morfemas em português, como desinências, raiz, radical, afixo, vogal temática e tema.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['Desinências', 'Raiz', 'Radical', 'Afixo', 'Vogal temática', 'Tema']}}], 'final_sentiment_count': {'positive': 0, 'negative': 0, 'neutral': 7, 'mixed': 0}, 'SSCORE': 0.0}, {'text': 'é o radical de “ beber ” , “ beberemos ” e “ bebendo ” , mas “ bebid ” seria o radical correto de “ bebida ” e de “ bebidinhas ” ) . Essa distinção , em termos linguísticos , é muito sutil e geralmente as aplicações de PLN assumem os dois termos como sinônimos . A raiz ou radical é o morfema nuclear de uma palavra , ou seja , aquele constituinte básico que expressa sua base significativa , que designa o significado lexical da palavra . Portanto , ele é o componente comum a todas as palavras que pertencem à mesma família . Por exemplo , em português , “ menino ” , “ meninas ” , “ meninada ” , “ meninice ” e outras possuem a mesma raiz ou radical “ menin ” . Os afixos são os morfemas lexicais que se juntam com o radical ou com o tema para formar uma nova palavra , neste caso chamada de palavra derivada . A inserção de um afixo ao radical de uma palavra pode mudar-lhe o sentido ou adicionar-lhe uma ideia secundária ou ainda mudar sua classe gramatical . Em português , os afixos podem ser de três tipos : Apesar de os sufixos e as desinências serem morfemas acoplados ao final da palavra , eles não devem ser confundidos , pois os primeiros criam novas palavras derivadas , a partir de um processo de formação de palavras chamado de derivação . Já a adição de desinências não cria novas palavras , apenas flexiona a palavra existente em uma nova forma flexionada . Vogal temática é o nome dado às vogais que aparecem imediatamente após o radical da palavra , mas não representam seu gênero . Em português , as vogais temáticas podem ser de dois tipos : As vogais temáticas não devem ser confundidas com as desinências nominais , por exemplo , que indicam gênero em “ menino ” e “ menina ” . Neste caso , “ o ” e “ a ” são desinências nominais porque esses morfemas marcam os gêneros masculino e feminino , respectivamente . Novamente , cabe mencionar que há uma boa e extensa discussão sobre a natureza e funcionamento das vogais temáticas em linguística . Para quem quiser uma visão aprofundada , vale consultar o trabalho de Santana ( 2019 ) , uma tese de doutorado sobre vogais temáticas . Tema é a forma lexical que se cria quando se juntam dois morfemas : o radical e a vogal temática . Por exemplo , a partir da combinação do radical “ crianç ” com a vogal temática nominal “ a ” , forma-se o tema “ criança ” . Embora seja a junção de dois tipos de morfemas , o tema também é considerado como um morfema . A forma lexical assumida pelo tema coincide com as formas de lexema , lexia e lema , que são as formas de entrada dos verbetes em dicionários , e que serão explicadas na Seção 4.1.3 . Ressalte-se , no entanto , que os termos tema e lema , na Linguística Textual , representam ideias completamente diferentes desses conceitos da morfologia . Por fim , vale dizer que todos esses tipos de morfemas explicados nas subseções anteriores podem ser agrupados em duas categorias : ( i ) morfemas lexicais , que representam a família semântica de determinada palavra , ou seja , a raiz , o radical e o tema ; ( ii ) morfemas gramaticais , que inserem alguma informação à palavra existente , ou seja , as desinências , os afixos e as vogais temáticas . Há ainda um tipo de morfema ( ou fonema , dependendo da abordagem ) que não foi explorado aqui porque não é relevante para os estudos de PLN , que são as vogais e as consoantes de ligação . Elas não possuem significado , mas , por vezes , são inseridas entre um radical e uma desinência ou um afixo , por uma motivação fonológica . Token é um termo que significa qualquer sequência de caracteres à qual se atribui um valor . Nas línguas europeias , a sequência consiste em caracteres delimitados por espaços gráficos , sendo que a tokenização é ajustada para separar sinais de pontuação . Mas , na grande maioria das línguas , a tokenização não opera por espaços gráficos . Diante dessa definição , é comum associarmos token à palavra escrita . Nesse sentido , a quantidade de palavras + sinais de pontuação de uma sentença equivale à quantidade de tokens , por exemplo , a frase do Exemplo 4.2 contém 12 tokens , já que , em PLN , os sinais de pontuação ( vírgula e ponto final ) também são considerados tokens2 . Exemplo 4.2 Eu sempre viajo para Campinas , para Salvador e para Belém . Type , por sua vez , refere-se aos tokens únicos encontrados numa frase ou texto . Retomando a frase do Exemplo 4.2 , encontramos 10 types ( “ eu ” , “ sempre ” , “ viajo ” , “ para ” , “ Campinas ” , “ , ” , “ Salvador ” , “ e ” , “ Belém ” e “ . ” ) . Nessa sentença , a palavra “ para ” ocorre três vezes , então ela é contada 3 vezes como token , mas apenas 1 vez como type . A proporção token/type ( divisão da quantidade de tokens pela quantidade de types ) é um importante indicativo da riqueza lexical de um texto ; ou seja , ela indica qual a diversidade de palavras existentes em um corpus , excluindo suas repetições . Mas , nessa medida , apenas as formas de palavras ( as palavras diferentes ) e o número total de palavras são contados . Isto é , sinais de pontuação não são considerados . Lexema é sinônimo de unidade lexical , o que implica características de som ,', 'summarized_text': 'O radical de uma palavra é o morfema nuclear que expressa sua base significativa. Os afixos são morfemas lexicais que se juntam ao radical para formar uma nova palavra. A vogal temática aparece imediatamente após o radical. O tema é a combinação do radical e da vogal temática. Os morfemas podem ser classificados como lexicais (raiz, radical, tema) ou gramaticais (desinências, afixos, vogais temáticas). A tokenização divide um texto em unidades significativas, chamadas tokens. O type refere-se aos tokens únicos em um texto. A proporção token/type indica a riqueza lexical do texto. O lexema é a forma de entrada de um verbete no dicionário.', 'sentence_results': [{'sentence': 'O radical de uma palavra é o morfema nuclear que expressa sua base significativa.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['radical', 'morfema nuclear', 'base significativa']}}, {'sentence': 'Os afixos são morfemas lexicais que se juntam ao radical para formar uma nova palavra.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['afixos', 'morfemas lexicais', 'radical', 'nova palavra']}}, {'sentence': 'A vogal temática aparece imediatamente após o radical.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['vogal temática', 'radical']}}, {'sentence': 'O tema é a combinação do radical e da vogal temática.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['radical', 'vogal temática', 'combinação']}}, {'sentence': 'Os morfemas podem ser classificados como lexicais (raiz, radical, tema) ou gramaticais (desinências, afixos, vogais temáticas).', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['lexicais', 'morfemas', 'classificados', 'raiz', 'radical', 'tema', 'gramaticais', 'desinências', 'afixos', 'vogais temáticas']}}, {'sentence': 'A tokenização divide um texto em unidades significativas, chamadas tokens.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['tokenização', 'texto', 'unidades significativas', 'tokens']}}, {'sentence': 'O type refere-se aos tokens únicos em um texto.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['type', 'tokens', 'únicos', 'texto']}}, {'sentence': 'A proporção token/type indica a riqueza lexical do texto.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['proporção', 'token', 'type', 'riqueza lexical', 'texto']}}, {'sentence': 'O lexema é a forma de entrada de um verbete no dicionário.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['lexema', 'verbete', 'dicionário']}}], 'final_sentiment_count': {'positive': 1, 'negative': 0, 'neutral': 8, 'mixed': 0}, 'SSCORE': 11.11111111111111}, {'text': 'forma e significado . Por exemplo , “ comprei ” é um lexema cuja representação fonética é [ kõpr ’ ej ] ; morfologicamente , é um verbo flexionado na primeira pessoa do singular , no pretérito perfeito do modo indicativo . Seu significado é o que encontramos nos dicionários : adquirir ( algo , produto , serviço etc . ) em troca de pagamento . Vale assinalar que , nos estudos do léxico do Brasil , temos também o termo técnico lexia , que corresponde à realização concreta de um lexema . Por exemplo , um lexema – que seria uma forma em abstrato , como “ árvore ” – pode acontecer sob a forma de uma lexia como “ árvores ” . Lexia é , nessa perspectiva , uma “ forma que um lexema assume no discurso . Exemplo : ‘ O dia está claro. ’ Temos aí quatro lexias ” ( Biderman , 1978 , p. 130 ) . A lexia realiza-se no discurso e/ou texto e se distingue do lexema , que se situa ao nível do sistema abstrato que é a língua . Em resumo , o lexema é uma representação conceitual enquanto a lexia é a unidade linguística materializada no discurso . Como você pode perceber , em linguística , temos vários termos para designar , algumas vezes , uma mesma noção . Por isso , um termo como palavra também equivale a vocábulo . Se quiser saber mais sobre essas diferentes concepções linguísticas , no âmbito dos estudos do léxico , vale dar uma olhada na parte introdutória do trabalho de Sarmento ( 2019 ) . Por sua vez , lema é a representação das propriedades sintático-semânticas de um item lexical . Isso significa que , a partir de um lema , é possível saber quais argumentos a ele se relacionam . Por exemplo , “ comprar ” é um verbo que seleciona dois argumentos : um sujeito e um objeto . Esses dois argumentos são necessários para que a estrutura na qual ele está inserido seja gramatical , ou seja , aceita e compreendida pelos falantes . Além disso , é por meio do lema que se pode acessar seu significado : “ comprar ” remete a uma ação que envolve uma moeda e a obtenção de algo . Nesse sentido , o lema pode ser considerado uma parte do lexema . A palavra , na forma de lema , é também a forma de entrada dos verbetes em um dicionário , tendo-se em mente também uma categoria de palavra . Por isso , temos “ filósofo ” como o lema dos substantivos “ filósofo ” , “ filósofos ” , “ filósofa ” e “ filósofas ” e temos “ filosofar ” como o lema de “ filosofei ” , “ filosofamos ” , “ filosofemos ” e todas as demais flexões do verbo . Se é verdade que não existe língua sem gramática , mais verdade ainda é que sem léxico não há língua . As palavras são a matéria-prima com que construímos nossas ações de linguagem . ( Antunes , 2017 , p. 27 ) A afirmação de Antunes ( 2017 ) na epígrafe diz muito e coloca em relação os elementos que estruturam e fazem funcionar uma língua . Um conjunto de regras sistemáticas servem para definir o que é considerado certo ou normal em uma língua . Por exemplo , segundo a regra de ordem de palavras , usamos , em um discurso normal , não poético , a frase “ ele leu o livro ” e não “ o leu ele livro ” . Regras como essa apontam para gramática , enquanto o léxico corresponde ao conjunto de palavras de uma língua . Léxicos contêm as palavras de uma língua juntamente com as definições morfossintáticas ( Seção 4.1.8 ) possíveis para cada uma das palavras . Geralmente cada palavra do léxico tem associada a ela uma ou mais triplas com sua categoria gramatical , também chamado de PoS ( Part-of-Speech ) , seu lema e suas características morfológicas , também chamadas de features . As categorias gramaticais podem variar segundo os critérios da representação que será adotada , podendo seguir um entre diversos padrões . Porém , para o português é usual definir as categorias gramaticais : substantivos , adjetivos , nomes próprios , numerais , pronomes , preposições , conjunções , advérbios e verbos . Dependendo dos critérios escolhidos , pode-se incluir outras categorias como artigos ou determinantes . Pode-se também promover divisões , como por exemplo dividir as conjunções em conjunções coordenativas e conjunções subordinativas , ou ainda verbos em verbos auxiliares e verbos plenos . A escolha do conjunto de categorias possíveis é a primeira decisão para a construção de um léxico . Um exemplo de categorias adotadas para o português é apresentado na Seção 4.3.3 . Igualmente , a escolha das características morfológicas que serão consideradas e seus valores possíveis é também uma decisão importante que deve ser tomada . É usual que a definição de categorias de PoS e características morfológicas seja acompanhada de um conjunto de etiquetas ( em inglês , tags ) que serão usadas para representar as informações associadas a cada palavra do léxico . Por exemplo , uma entrada de um léxico para a palavra “ elas ” pode conter a categoria gramatical pronome , o lema “ ele ” e características de pronome pessoal na terceira pessoa do plural e gênero feminino . Neste exemplo , trata-se de uma palavra que só possui uma possível tripla PoS , lema e features . No entanto , é bastante comum encontrarmos palavras que possuem diversas triplas possíveis de informações associadas , como por exemplo , a palavra “ casas ” que pode ser : Cabe salientar que pela própria natureza das línguas , por mais completo que um léxico possa ser , sempre é possível ter palavras da língua ausentes do léxico . O léxico comum corresponde ao', 'summarized_text': 'Um lexema é uma forma abstrata de uma palavra, enquanto uma lexia é a forma concreta que um lexema assume no discurso. O lema é a representação das propriedades sintático-semânticas de um item lexical. O léxico corresponde ao conjunto de palavras de uma língua, juntamente com suas definições morfossintáticas. O léxico comum contém as palavras mais comumente usadas em uma língua, mas sempre é possível ter palavras ausentes do léxico.', 'sentence_results': [{'sentence': 'Um lexema é uma forma abstrata de uma palavra, enquanto uma lexia é a forma concreta que um lexema assume no discurso.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['lexema', 'lexia']}}, {'sentence': 'O lema é a representação das propriedades sintático-semânticas de um item lexical.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['lema', 'propriedades', 'sintático-semânticas', 'item lexical']}}, {'sentence': 'O léxico corresponde ao conjunto de palavras de uma língua, juntamente com suas definições morfossintáticas.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['léxico', 'palavras', 'definições', 'morfossintáticas']}}, {'sentence': 'O léxico comum contém as palavras mais comumente usadas em uma língua, mas sempre é possível ter palavras ausentes do léxico.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['comum', 'contém', 'palavras', 'mais', 'usadas', 'língua', 'sempre', 'possível', 'ausentes']}}], 'final_sentiment_count': {'positive': 0, 'negative': 0, 'neutral': 4, 'mixed': 0}, 'SSCORE': 0.0}, {'text': 'conjunto de palavras de uma língua que não têm um “ conceito técnico-científico ” bem determinado , historicamente construído , atrelado a ela . Em contrapartida há o léxico especializado , no qual a palavra assume um significado específico/especial em relação a um sistema de conceitos específico , que geralmente corresponde a uma área de conhecimento , ciência ou especialidade . É esse ambiente “ especializado ” que definirá se ela pode ser entendida como uma terminologia “ técnica ” ( termo ) ou uma palavra comum . Um item que a gente lê e diz que é um termo é , por exemplo , “ ferritina ” , enquanto “ caderno ” parece um protótipo de palavra comum , do léxico comum . Novamente , pode-se pensar que a categorização ou classificação são referências e que sempre pode haver algo que parece um meio-termo . E há ainda termos técnicos que passam a funcionar como palavras , na língua comum , e vice-versa . Um exemplo interessante é o caso de “ criança ” , que no âmbito jurídico corresponde a uma pessoa que tem até doze anos de idade , conforme vemos no Estatuto da Criança e do Adolescente ( ECA ) 3 do Brasil . Isto é , nesse “ cenário especializado ” do ECA , a palavra “ criança ” assume status de terminologia , pois corresponde a um conceito específico , oposto a outros . Já a palavra “ acetona ” , como sinônimo de “ removedor de esmalte de unhas ” , é algo que fez o caminho inverso , uma vez que passou de termo do léxico especializado a palavra do léxico comum . Veja como fica o termo “ DNA ” , que é uma sigla para um termo “ técnico ” em inglês , nome de um ácido , que é usado em diferentes situações e parece circular entre o ambiente técnico Exemplo 4.4 e o ambiente da linguagem comum Exemplo 4.3 , do nosso dia a dia – pois passou a corresponder a um nome de um exame para confirmação de paternidade . Exemplo 4.3 No programa de TV , Joana disse que ia jogar na cara do ex-namorado um DNA . E disse que ele , depois , ia ver que o filho que ele renegou tem um DNA de gente de bem . Exemplo 4.4 O DNA ( ácido desoxirribonucleico ) é um tipo de ácido nucleico que possui papel fundamental na hereditariedade , sendo considerado o portador da mensagem genética.4 Vale mencionar , também , que para aplicações que envolvem mais do que um idioma , como a Tradução Automática ( Capítulo 18 ) , os léxicos são bilíngues ( ou multilíngues ) especificando não apenas as palavras que compõem os léxicos dos vários idiomas , mas também o mapeamento ( paralelismo ) existente entre palavras de um e outro ( s ) idioma ( s ) . As palavras funcionais/gramaticais e as palavras lexicais são outra dualidade , também complexa , que podemos tentar “ resolver ” ou melhor , entender , pensando em classificá-las . As palavras funcionais/gramaticais ficam em uma classe fechada . Já as palavras lexicais ficam em outro grupo ou tipo , pensando que correspondem a uma classe aberta . A classe fechada é assim pensada porque tem um número finito de componentes . A classe aberta , por outro lado , acomoda um número bem maior de componentes , pois é uma classe que tem a ver com a capacidade de as pessoas criarem palavras novas . Podemos pensar que as preposições do português são as mesmas desde sempre ; não criamos muitas . Já os adjetivos e os substantivos não param de nos surpreender , pois parece que há uma inventividade envolvida em nomes e qualificativos , como o adjetivo , que também pode ser substantivo “ cloroquiner ” . Essa nova palavra surgiu no contexto da Pandemia de Covid-19 , em 2020 , no Brasil . Pensar em conjuntos também nos ajuda a entender essa diferença entre funcional/gramatical e lexical . Mas sempre poderemos pensar que uma palavra como “ não ” é uma palavra lexical , se o critério para classificar for “ palavra que tem um sentido ” em si mesma . Via de regra , algumas classes de palavras são sempre consideradas como de classe aberta , como os verbos , os adjetivos e os substantivos , enquanto outras classes são sempre definidas como de classe fechada , tais como os artigos ( determinantes ) , as preposições e as conjunções . Outras classes , como os advérbios ou os pronomes , por exemplo , podem ser considerados palavras lexicais ou funcionais , dependendo de suas subclassificações . Existem dois tipos de processos usados para a formação de novas palavras : ( i ) por derivação e ( ii ) por composição . São mecanismos linguísticos que permitem criar novas palavras a partir de unidades já existentes na língua . A derivação é um processo pelo qual novas palavras são criadas adicionando afixos ( prefixos , sufixos , infixos etc . ) à raiz ou radical . Esses afixos podem alterar o significado , a classe gramatical ( substantivo , adjetivo , verbo etc . ) ou outros aspectos da palavra base . Por exemplo , considere o substantivo “ amigo ” . Se adicionarmos o sufixo “ -ável ” a ele , obtemos o adjetivo “ amigável ” . Nesse caso , o sufixo altera o sentido da palavra e também sua classe gramatical . Existem cinco tipos de derivação : Já a composição é um processo em que novas palavras são formadas combinando duas ou mais palavras independentes , ou dois radicais , para criar uma nova palavra com um significado diferente . As palavras compostas podem ser formadas por substantivos , adjetivos , verbos , advérbios e outras classes gramaticais . Além disso , elas podem ser escritas juntas , separadas por hífen', 'summarized_text': 'O léxico de uma língua consiste nas palavras comuns que não possuem um conceito técnico-científico específico, enquanto o léxico especializado contém termos que têm significados específicos em áreas de conhecimento ou ciência. Alguns termos técnicos podem se tornar palavras comuns e vice-versa. As palavras funcionais/gramaticais são um grupo fechado, enquanto as palavras lexicais são um grupo aberto que permite a criação de novas palavras. A formação de novas palavras pode ocorrer por derivação ou composição.', 'sentence_results': [{'sentence': 'O léxico de uma língua consiste nas palavras comuns que não possuem um conceito técnico-científico específico, enquanto o léxico especializado contém termos que têm significados específicos em áreas de conhecimento ou ciência.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['língua', 'léxico', 'palavras', 'conceito', 'técnico', 'científico', 'específico', 'léxico especializado', 'termos', 'áreas de conhecimento', 'ciência']}}, {'sentence': 'Alguns termos técnicos podem se tornar palavras comuns e vice-versa.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['API', 'big data', 'cloud computing', 'machine learning', 'blockchain']}}, {'sentence': 'As palavras funcionais/gramaticais são um grupo fechado, enquanto as palavras lexicais são um grupo aberto que permite a criação de novas palavras.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['palavras funcionais', 'palavras gramaticais', 'grupo fechado', 'palavras lexicais', 'grupo aberto', 'novas palavras']}}, {'sentence': 'A formação de novas palavras pode ocorrer por derivação ou composição.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['formação', 'novas palavras', 'derivação', 'composição']}}], 'final_sentiment_count': {'positive': 0, 'negative': 0, 'neutral': 4, 'mixed': 0}, 'SSCORE': 0.0}, {'text': 'ou até mesmo separadas sem qualquer marcação , dependendo da língua e das convenções ortográficas . São exemplos de palavras formadas por composição : “ girassol ” ( “ gira ” + “ sol ” ) , “ planalto ” ( “ plano ” + “ alto ” ) , “ guarda-chuva ” ( “ guarda ” + “ - ” + “ chuva ” ) . Existem 2 tipos de composição : Ambos os processos de derivação e de composição são fundamentais para a expansão do vocabulário e a expressão de nuances semânticas na linguagem . Por fim , mas não menos importante , é necessário definir o escopo de estudo do que chamamos de morfologia , pois o seu objeto de estudo muitas vezes se intersecta com o objeto de outra área da linguística , a chamada Sintaxe , que será explorada no Capítulo 6 . Na fronteira entre a morfologia e a sintaxe , está a morfossintaxe . Na prática , essas três áreas estão intimamente ligadas e relacionadas , mas , para fins didáticos , distinguimos esses termos a partir de seus objetos de estudo . A morfologia é o ramo da linguística que se concentra no estudo dos morfemas , que são os “ pedacinhos ” significativos que formam as palavras . Assim , a morfologia examina como eles se combinam nos processos de flexão e de formação de palavras . Em PLN , a morfologia cuida também da classificação dos atributos morfológicos ( ou features morfológicas ) , tais como os traços de gênero , número , modo , tempo , pessoa , voz , caso , entre outros . Já a morfossintaxe examina como as escolhas morfológicas ( como flexões verbais e concordância nominal ) afetam a organização das palavras em uma sentença e como essas escolhas influenciam a estrutura sintática . Em outras palavras , ela categoriza as palavras em diferentes classes de palavras ( ou categorias gramaticais ) a partir da observação de seus atributos morfológicos . Em PLN , as classes de palavras são chamadas de part-of-speech ou PoS e a tarefa de atribuição de etiquetas de PoS nos textos será explicada na Seção 4.2.5 . Em resumo , a morfologia lida com a estrutura interna das palavras e os morfemas que as compõem , enquanto a morfossintaxe explora como as escolhas morfológicas afetam a estrutura das frases . Ambas as áreas são consideradas neste capítulo . Após definir conceitos necessários da área de morfologia , demonstraremos como tratar esse nível de análise linguística no Processamento de Linguagem Natural . Para desenvolver praticamente qualquer aplicação de PLN , é necessário realizar fases/etapas que convencionamos chamar de pré-processamento . Nesse pré-processamento , algumas tarefas usuais são : segmentação do texto em sentenças ( sentenciação ) , separação de palavras ( tokenização ) , tokenização em subpalavras ( vetorização de subtokens ) , normalização de palavras ( lematização e radicalização ) , entre outras . Além das etapas do pré-processamento , também podem ser realizadas tarefas de processamento do conteúdo dos textos , como a etiquetagem morfossintática das palavras em relação às suas classes gramaticais ( tarefa de PoS tagging ) e a anotação automática de seus atributos morfológicos ( tarefa de anotação de feats ou features morfológicas ) , que também serão exploradas nesta seção . A sentenciação ( ou sentenciamento ) é o processo de segmentação do texto em sentenças , ou seja , é o processo de identificação de unidades textuais de processamento onde se definem os limites de cada sentença . A denominação de detecção de limite de sentença é frequentemente utilizada como sinônimo da segmentação de sentenças , pois o problema se limita a descobrir onde cada sentença termina ( Hapke ; Howard ; Lane , 2019 ) . Este processo é naturalmente complexo , pois a ambiguidade das línguas torna impossível ter sempre certeza de onde termina uma sentença ( Read et al. , 2012 ) . No caso do português escrito , as técnicas usuais se valem da busca de pontuações delimitadoras como “ . ” , “ ! ” e “ ? ” . Note-se que no processamento de textos falados , ou mesmo em algumas línguas onde a delimitação de sentenças não é feita por pontuação , o processo de segmentação de sentenças se torna ainda mais difícil . A detecção do limite de sentença no português não tem como desafio identificar as pontuações delimitadoras , pois esse é usualmente um conjunto finito e conhecido ( “ . ” , “ ! ” e “ ? ” , “ ... ” ) . O desafio é desambiguar essas ocorrências com outros usos dos mesmos caracteres . Um exemplo disto é o caso das abreviações . Por exemplo , na sentença do Exemplo 4.5 temos duas ocorrências do caractere “ . ” . Exemplo 4.5 Fui à clínica do Dr. Nilo . Na primeira ocorrência , o “ . ” é utilizado como indicador da abreviação da palavra “ Doutor ” e , na segunda , como delimitador do fim da sentença . Note-se que a sentença do Exemplo 4.6 é uma sentença perfeitamente aceitável e , neste caso , o “ . ” está sendo utilizado duplamente como indicador de abreviação e fim de sentença . Exemplo 4.6 Fui à clínica do Dr. Outro caso comum de ambiguidade no uso de pontuações delimitadoras é encontrado em numerais . Em português , utiliza-se também o caractere “ . ” como delimitador de milhar em um número , enquanto em inglês ele é utilizado como separador de decimais . Algumas vezes ambos os usos aparecem , ainda que erroneamente , em sentenças em português , como no Exemplo 4.7 . Exemplo 4.7 A venda de 25.000 ações fez o índice de rentabilidade baixar para 0.5 % , segundo a BOVESPA . Um problema semelhante acontece com algumas definições matemáticas dentro de uma sentença , como quando se utiliza o caractere “ ! ” para definir', 'summarized_text': 'A morfologia é o ramo da linguística que estuda os morfemas e como eles se combinam para formar as palavras. A morfossintaxe, por sua vez, explora como as escolhas morfológicas afetam a estrutura das frases. Ambas as áreas são importantes no processamento de linguagem natural. No pré-processamento de texto, tarefas como segmentação de sentenças, tokenização e etiquetagem morfossintática são realizadas. A sentenciação é a segmentação do texto em sentenças, que pode ser feita usando pontuações delimitadoras. No entanto, a ambiguidade das línguas e o uso de pontuações em abreviações e numerais tornam o processo desafiador.', 'sentence_results': [{'sentence': 'A morfologia é o ramo da linguística que estuda os morfemas e como eles se combinam para formar as palavras.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['morfologia', 'linguística', 'morfemas', 'palavras']}}, {'sentence': 'A morfossintaxe, por sua vez, explora como as escolhas morfológicas afetam a estrutura das frases.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['morfossintaxe', 'escolhas morfológicas', 'estrutura das frases']}}, {'sentence': 'Ambas as áreas são importantes no processamento de linguagem natural.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['processamento de linguagem natural', 'NLP', 'análise de sentimentos', 'extração de informações', 'classificação de texto', 'reconhecimento de entidades', 'sumarização de texto', 'tradução automática', 'busca de informações', 'processamento de fala', 'processamento de texto', 'modelagem de linguagem', 'algoritmos de aprendizado de máquina']}}, {'sentence': 'No pré-processamento de texto, tarefas como segmentação de sentenças, tokenização e etiquetagem morfossintática são realizadas.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['pré-processamento de texto', 'segmentação de sentenças', 'tokenização', 'etiquetagem morfossintática']}}, {'sentence': 'A sentenciação é a segmentação do texto em sentenças, que pode ser feita usando pontuações delimitadoras.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['sentenciação', 'segmentação', 'texto', 'sentenças', 'pontuações delimitadoras']}}, {'sentence': 'No entanto, a ambiguidade das línguas e o uso de pontuações em abreviações e numerais tornam o processo desafiador.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['ambiguidade', 'línguas', 'pontuações', 'abreviações', 'numerais', 'processo', 'desafiador']}}], 'final_sentiment_count': {'positive': 0, 'negative': 0, 'neutral': 6, 'mixed': 0}, 'SSCORE': 0.0}, {'text': 'o fatorial de um número , como no Exemplo 4.8 . Exemplo 4.8 As permutações de cinco elementos podem ser calculadas como 5 ! , que é o fatorial de cinco . Em todos estes casos , torna-se difícil detectar quando o caractere está sendo utilizado com função de fim de sentença ou não . Por essas razões , o problema de segmentação automática de textos , ainda que explorado desde o início pela área de PLN , é bastante desafiador e ainda está em aberto . Atualmente utilizam-se três tipos de abordagens computacionais para resolvê-lo : O problema de segmentação de sentenças é extremamente importante , pois , por ser uma etapa inicial do pré-processamento , os problemas não resolvidos nessa etapa tendem a prejudicar as etapas posteriores . Em comparação com outras tarefas de PLN , a segmentação de sentenças costuma receber menos atenção do que deveria , tanto no desenvolvimento de pesquisas , quanto na implementação de casos práticos . Vale esclarecer que a sentenciação é uma tarefa de pré-processamento que tem relação com a morfologia porque , apesar de sua unidade de análise ser a sentença , os casos de ambiguidade ( e , portanto , segmentação incorreta ) têm a ver com a delimitação das palavras , que é o objeto de estudo da morfologia . Na Seção 4.3.1 , indicaremos alguns sentenciadores ( ou também chamados sentencizers ) disponíveis para o português . A separação em unidades linguísticas mínimas é denominada tokenização ( em inglês , tokenization ) e , como já mencionado anteriormente , no caso do português é feita partindo da separação das palavras através de delimitadores . Neste caso , faz-se necessário identificar os limites das palavras através de caracteres delimitadores como espaços em branco ou símbolos de pontuação como “ , ” , “ : ” , “ ; ” , “ - ” e “ . ” . Novamente , aqui é necessário se atentar para casos específicos como “ , ” , “ - ” e “ . ” que não devem ser separados dos demais caracteres que vêm antes ou depois . Por exemplo , a sentença do Exemplo 4.9 possui 11 tokens : “ Além ” , “ disso ” , “ , ” , “ a ” , “ produção ” , “ será ” , “ descontinuada ” , “ em ” , “ 8,3 ” , “ % ” e “ . ” , sendo que 8,3 deve ser considerado um token único e não 3 tokens separados . Exemplo 4.9 Além disso , a produção será descontinuada em 8,3 % . Outra tarefa frequente da tokenização é a separação de palavras contraídas , por exemplo , a palavra “ da ” é separada em dois tokens : “ de ” + “ a ” e a palavra “ nelas ” é separada nos tokens “ em ” + “ elas ” . Essa tarefa é necessária para diversas aplicações e , em muitos casos , é um processo simples , pois a palavra contraída não é ambígua . No entanto , em alguns casos , pode ser necessário um processo de desambiguação , como nas palavras “ pelo ” ( que pode ser “ por ” + “ o ” ou o substantivo “ pelo ” ) e “ consigo ” ( que pode ser tokenizada em “ com ” + “ si ” ou corresponder à conjugação do verbo “ conseguir ” ) . Esse é ainda um dos desafios da tokenização em português . A Figura 4.1 mostra o resultado da tokenização usando uma das ferramentas atualmente disponíveis para o português6 para a sentença do Exemplo 4.10 . Exemplo 4.10 A raça Lulu da Pomerânia solta pelos em abundância pela casa . Na Figura 4.1 observamos que “ da ” e “ pela ” foram corretamente descontraídas como “ de_ a ” e “ por_ a ” respectivamente . Já o substantivo “ pelos ” foi incorretamente separado em “ por_ os ” . Problemas de descontração indevida , ou da falta dela quando necessária , geram problemas para a análise sintática , como veremos no Capítulo 6 . Além desses casos , há também outros que envolvem a decisão de separar ou não palavras hifenizadas . Por exemplo , usualmente não se separa palavras como “ sexta-feira ” , que correspondem a um substantivo único . No entanto , é usual tokenizar palavras como “ sinto-me ” em três tokens : o verbo “ sinto ” , o sinal de pontuação “ - ” e o pronome “ me ” . Mais uma vez a aplicação pretendida é que definirá o que deve ser feito ou não . A tokenização , assim como o sentenciamento , é um processo que pode ser resolvido com estratégias baseadas em regras ou que utilizam as mesmas abordagens de aprendizado de máquina supervisionadas e não supervisionadas citadas anteriormente . A complexidade do processo de tokenização é , no entanto , menor que a do processo de sentenciação , pois o processo pode ser auxiliado pela existência de recursos léxicos que facilitam bastante a tarefa de identificar os limites possíveis da maioria dos tokens a serem separados . Na Seção 4.3.1 , apresentaremos os tokenizadores ( ou também chamados tokenizers ) disponíveis para o português . Outro conceito relacionado ao de unidade de processamento que se tornou bastante popular nos últimos tempos ( principalmente com o surgimento das arquiteturas neurais para processamento da língua ) é o de subpalavra ( em inglês , subword ) . As aplicações recentes de modelos de linguagem baseados em redes neurais tornaram bastante comum a quebra de palavras em porções eventualmente menores , as subpalavras . Esse processo de tokenização em subpalavras tem por objetivo reduzir o vocabulário de trabalho de um modelo de linguagem a um tamanho finito , mas que possa ser usado para representar textos onde o número de types ( quantidade de tokens distintos', 'summarized_text': 'O problema de segmentação automática de textos, ainda que explorado desde o início pela área de PLN, é bastante desafiador e ainda está em aberto. Atualmente utilizam-se três tipos de abordagens computacionais para resolvê-lo: regras, aprendizado de máquina supervisionado e aprendizado de máquina não supervisionado. A tokenização é um dos processos importantes no pré-processamento de textos, que consiste em separar o texto em unidades linguísticas mínimas, chamadas de tokens. Essa tarefa pode ser feita utilizando regras ou abordagens de aprendizado de máquina. Além disso, o conceito de subpalavra também é utilizado, principalmente em modelos de linguagem baseados em redes neurais, para reduzir o tamanho do vocabulário de trabalho.', 'sentence_results': [{'sentence': 'O problema de segmentação automática de textos, ainda que explorado desde o início pela área de PLN, é bastante desafiador e ainda está em aberto.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['segmentação automática de textos', 'PLN', 'desafiador', 'em aberto']}}, {'sentence': 'Atualmente utilizam-se três tipos de abordagens computacionais para resolvê-lo: regras, aprendizado de máquina supervisionado e aprendizado de máquina não supervisionado.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['regras', 'aprendizado de máquina supervisionado', 'aprendizado de máquina não supervisionado']}}, {'sentence': 'A tokenização é um dos processos importantes no pré-processamento de textos, que consiste em separar o texto em unidades linguísticas mínimas, chamadas de tokens.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['tokenização', 'processo', 'pré-processamento', 'textos', 'unidades linguísticas', 'mínimas', 'tokens']}}, {'sentence': 'Essa tarefa pode ser feita utilizando regras ou abordagens de aprendizado de máquina.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['regras', 'abordagens', 'aprendizado de máquina']}}, {'sentence': 'Além disso, o conceito de subpalavra também é utilizado, principalmente em modelos de linguagem baseados em redes neurais, para reduzir o tamanho do vocabulário de trabalho.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['subpalavra', 'modelos de linguagem', 'redes neurais', 'vocabulário']}}], 'final_sentiment_count': {'positive': 0, 'negative': 0, 'neutral': 5, 'mixed': 0}, 'SSCORE': 0.0}, {'text': ') seja potencialmente infinito . Dessa forma , pode-se utilizar um conjunto de treino que possua um vocabulário finito e que seja capaz de ser aplicado a qualquer texto7 . Por exemplo , um modelo de linguagem pode ser projetado para ter um vocabulário de trabalho com 30.000 types , ou seja , um índice numérico de 1 a 30.000 único para cada type a ser representado de forma que se possa definir um vetor de 30.000 posições , em que cada posição corresponde a um dos types considerados . Essa abordagem é necessária para utilizar representações vetoriais como , por exemplo , as representações word2vec ( Mikolov et al. , 2013 ) e GloVe ( Pennington ; Socher ; Manning , 2014 ) a serem tratadas no Capítulo 10 . Apesar dos vocabulários de trabalho serem usualmente grandes , esses vocabulários são insuficientes para representar textos em português associando uma possível palavra da língua a cada type , já que o português possui mais de 800.000 palavras , sem contar novas palavras que podem ser criadas , como o exemplo “ cloroquiner ” citado na Seção 4.1.6 . A abordagem de tokenização em subpalavras consiste em codificar diretamente algumas palavras mais comuns , como “ de ” , “ fazer ” , “ são ” e “ feliz ” . No entanto , palavras mais raras , como “ desfazer ” ou “ felizmente ” podem ficar fora do vocabulário de trabalho ( OOV do termo em inglês out-of-vocabulary ) e portanto serem representadas como combinações de subpalavras , respectivamente : “ de ” + “ s ” + “ fazer ” e “ feliz ” + “ mente ” . Dessa forma , uma subpalavra pode ser uma sequência qualquer de caracteres ou podem ser sequências com algum significado linguístico como prefixos e sufixos ( e.g. , “ mente ” comumente designa advérbios , como “ felizmente ” ) , mas até letras únicas de forma que sempre seja possível representar qualquer palavra pela composição de subpalavras pertencentes ao vocabulário ( e.g. , a subpalavra “ s ” , que é uma letra única , pode ser utilizada para , junto com as subpalavras “ de ” e “ fazer ” , ser articulada para representar “ de ” + “ s ” + “ fazer ” ) . A escolha do vocabulário de trabalho , ou seja , a escolha das subpalavras que o compõem , pode ser feita utilizando diversas técnicas , mas três algoritmos são frequentemente utilizados : BPE ( Byte-Pair Encoding ) ( Sennrich ; Haddow ; Birch , 2016 ) , Word-Piece ( Schuster ; Nakajima , 2012 ) e Unigram ( Kudo , 2018 ) . O primeiro , BPE , é inspirado em técnicas de compreensão de dados e busca representar como subpalavras os tokens mais frequentes . O segundo , Word-Piece , é utilizado pelo BERT ( Devlin et al. , 2019 ) e busca reduzir o tamanho do vocabulário através da escolha de subpalavras que possam ser utilizadas em um número maior de tokens . O terceiro , Unigram , também foca na redução do vocabulário e inicializa o treinamento com um vocabulário grande de caracteres , subpalavras e palavras , e vai reduzindo esse vocabulário mantendo somente os itens mais relevantes até alcançar o tamanho de vocabulário desejado . Por sua vez , a normalização é a tarefa que converte as palavras para alguma forma padrão . São exemplos de normalização : conversão de versões abreviadas de palavras ( e.g. , conversão de “ vc ” para “ você ” ) , conversão para caracteres minúsculos ( e.g. , convertendo “ Você ” para “ você ” ) , lematização ( e.g. , estabelecendo que “ somos ” é uma conjugação do verbo “ ser ” ) e radicalização ( e.g. , estabelecendo que “ retrabalho ” tem o radical “ trabalho ” 8 precedido do prefixo “ re ” ) . De acordo com a abordagem utilizada , diferentes tipos de normalização podem ser necessários para tratar de maneira mais eficiente o processamento textual . Na verdade , o propósito do tratamento computacional tem muita influência nos tipos de normalização que precisam ser feitos . Em alguns casos , certas informações no texto a ser processado podem ser vistas como relevantes , enquanto outras como apenas ruído . Por exemplo , quando temos como propósito identificar o sentido geral de um texto , a conversão de abreviaturas tende a auxiliar a identificação do conteúdo considerando , por exemplo , “ PLN ” e “ Processamento de linguagem natural ” como equivalentes . Por outro lado , com o propósito de identificar entidades nomeadas , a conversão para caracteres minúsculos pode dificultar o processamento . A tarefa de conversão de abreviações é usualmente baseada em listas predefinidas de abreviações comuns que podem ser utilizadas em um processo de busca e substituição . No entanto , alguns cuidados usuais devem ser tomados para que somente tokens completos e com o sentido apropriado sejam substituídos . Por exemplo , uma abreviação usual em textos de mensagens é a representação “ rs ” como abreviação de “ risos ” , mas a substituição desta abreviação , que está correta na sentença do Exemplo 4.11 , ficaria completamente errada na sentença do Exemplo 4.12 , pois poderia resultar em “ Os youtuberisos do RS tem muito sotaque. ” que substituiria erroneamente o final da palavra “ youtubers ” . Exemplo 4.11 Eu já sabia… rs . Exemplo 4.12 Os youtubers do RS tem muito sotaque . A tarefa de conversão para caracteres minúsculos é menos delicada , podendo ser facilmente implementada por simples processamento da representação individual dos caracteres . No entanto , mesmo nesse caso é necessário tomar certas precauções com nomes próprios e outras representações onde existe semântica associada ao uso de maiúsculas e minúsculas . Por exemplo , se fizermos a substituição de maiúsculas por minúsculas da frase do', 'summarized_text': 'O texto descreve a necessidade de representar textos em linguagem natural de forma vetorial, utilizando um conjunto de treinamento com um vocabulário finito. Para representar palavras em português, que possui mais de 800.000 palavras, podem ser utilizadas subpalavras como prefixos e sufixos. Existem três algoritmos frequentemente utilizados para escolher as subpalavras que compõem o vocabulário de trabalho: BPE, Word-Piece e Unigram. A normalização é a tarefa de converter as palavras para uma forma padrão, como a conversão de abreviações, caracteres minúsculos, lematização e radicalização. A escolha da normalização depende do propósito de processamento textual. A conversão de abreviações é baseada em listas predefinidas, mas cuidados devem ser tomados para evitar substituições incorretas. A conversão para caracteres minúsculos pode ser facilmente implementada, mas é necessário levar em conta a semântica associada ao uso de maiúsculas e minúsculas.', 'sentence_results': [{'sentence': 'O texto descreve a necessidade de representar textos em linguagem natural de forma vetorial, utilizando um conjunto de treinamento com um vocabulário finito.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['representar', 'textos', 'linguagem natural', 'vetorial', 'conjunto de treinamento', 'vocabulário finito']}}, {'sentence': 'Para representar palavras em português, que possui mais de 800.000 palavras, podem ser utilizadas subpalavras como prefixos e sufixos.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['prefixo', 'subpalavra', 'sufixo']}}, {'sentence': 'Existem três algoritmos frequentemente utilizados para escolher as subpalavras que compõem o vocabulário de trabalho: BPE, Word-Piece e Unigram.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['BPE', 'Word-Piece', 'Unigram']}}, {'sentence': 'A normalização é a tarefa de converter as palavras para uma forma padrão, como a conversão de abreviações, caracteres minúsculos, lematização e radicalização.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['normalização', 'palavras', 'forma padrão', 'abreviações', 'caracteres minúsculos', 'lematização', 'radicalização']}}, {'sentence': 'A escolha da normalização depende do propósito de processamento textual.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['normalização', 'processamento textual', 'propósito']}}, {'sentence': 'A conversão de abreviações é baseada em listas predefinidas, mas cuidados devem ser tomados para evitar substituições incorretas.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['abreviações', 'listas predefinidas', 'substituições']}}, {'sentence': 'A conversão para caracteres minúsculos pode ser facilmente implementada, mas é necessário levar em conta a semântica associada ao uso de maiúsculas e minúsculas.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['conversão', 'caracteres minúsculos', 'semântica', 'maiúsculas', 'minúsculas']}}], 'final_sentiment_count': {'positive': 0, 'negative': 0, 'neutral': 7, 'mixed': 0}, 'SSCORE': 0.0}, {'text': 'Exemplo 4.12 , podemos erroneamente descaracterizar a denominação do estado do Rio Grande do Sul ( “ RS ” ) , tornando o texto produzido ambíguo com a abreviação “ rs ” . Apesar disso , o processo de conversão para caracteres minúsculos é uma tarefa usual , pois permite a consulta a recursos linguísticos de uma forma mais eficiente . A tarefa de lematização envolve , frequentemente , a consulta a recursos linguísticos que possuem a definição de lemas e morfologia das palavras , como , por exemplo , um léxico da língua ( Seção 4.3.2 ) . O grande desafio desta tarefa é a desambiguação sintática das palavras que , segundo seu uso , podem ter lemas distintos . Por exemplo , na sentença do Exemplo 4.13 a palavra “ casa ” na sua primeira ocorrência terá como lema o verbo “ casar ” , enquanto a segunda ocorrência terá como lema o substantivo “ casa ” . Nesses casos é importante desambiguar morfossintaticamente as palavras ( Lopes et al. , 2023 ) . Exemplo 4.13 Quem casa , quer casa . Outra abordagem utilizada é através de padrões , a fim de trazer a palavra para sua forma canônica ( e.g. , trazer substantivos para o masculino singular e todas as flexões do verbo para sua forma no infinitivo ) ( Bertaglia ; Nunes , 2016 ) . Note-se que essa abordagem requer sofisticações para tratar palavras que não têm comportamento regular . Por exemplo , se a palavra “ meninas ” pode ser trazida corretamente ao lema substituindo a terminação “ as ” por “ o ” resultando no lema “ menino ” , a palavra “ casas ” seria erroneamente lematizada para o lema “ caso ” se utilizássemos o mesmo princípio . A tarefa de radicalização tem o propósito de converter lexemas para seus radicais . Uma particular vantagem deste tipo de tarefa é uniformizar e diminuir o vocabulário , ainda que possam levar à perda de informação . No entanto , em algumas aplicações , a busca dos radicais de uma palavra pode auxiliar , inclusive , no estabelecimento de subpalavras que foi descrito na Seção 4.2.3 . Por exemplo , as palavras “ certo ” , “ certidão ” , “ incerto ” , “ certamente ” , “ certificação ” , “ certeiro ” e “ incerteza ” possuem o mesmo radical “ cert ” e , portanto , tornam “ cert ” um bom candidato a subpalavra . Algoritmos de radicalização ( stemming , em inglês ) podem ser encontrados em bibliotecas usuais da área de PLN , como NLTK ( Bird ; Loper , 2004 ) , que oferecem opções para várias línguas , inclusive para o português . Na Seção 4.3.1 , apresentaremos algumas ferramentas disponíveis para o português que fazem a normalização dos textos , tais como lematizadores , radicalizadores , stemmers e outros . O PoS tagging , também conhecido como etiquetagem morfossintática , é uma técnica fundamental na área de PLN que envolve a atribuição de etiquetas gramaticais a cada palavra em um texto , com base na sua classe gramatical e em suas características morfológicas . Essas etiquetas ajudam a identificar a função sintática e morfológica das palavras em uma sentença , o que é crucial para a posterior análise sintática , mas também é útil para outras aplicações , como tradução automática ( Capítulo 18 ) , análise de sentimentos , geração de resumos , entre outras . As classes de palavras são universais e valem para a grande maioria das línguas naturais , incluindo o português . São elas : substantivos , verbos , adjetivos , advérbios , pronomes , numerais , artigos , conjunções , preposições e interjeições . Porém , as etiquetas de PoS que cada modelo de anotação define podem ser diferentes , assim como pode haver diferentes níveis de granularidade das etiquetas . Nesse sentido , um tagger ( nome dado a uma ferramenta computacional que realizada a etiquetagem morfossintática ) pode usar apenas as etiquetas de granularidade grossa ( em inglês , coarse tags ou UPoS ) , que correspondem às classes de palavras acima , enquanto outros taggers podem usar um conjunto de etiquetas de granularidade fina ( em inglês , fine-grained tags ou XPoS ) . Por exemplo , os adjetivos costumam ser associados à coarse tag ADJ , mas também podem ser etiquetados como JJ ( para adjetivos primitivos ) , JJR ( para adjetivos comparativos ) ou JJS ( para adjetivos superlativos ) . Horsmann ; Zesch ( 2016 ) propõem uma abordagem que combina os dois níveis de anotação para aumentar a precisão do tagger . A Figura 4.2 mostra um exemplo de etiquetagem morfossintática realizada pelo parser LXUTagger9 . Cada palavra é analisada quanto à sua forma e função na sentença . É necessário fazer essa análise dependente de contexto porque existem palavras polissêmicas , ambíguas , homônimas etc . Dentro do contexto , é possível , por exemplo , distinguir o substantivo “ ajuda ” ( e.g . “ Ele recebeu ajuda para executar o trabalho. ” ) do verbo “ ajuda ” ( e.g . “ Sempre que precisa , ele ajuda a comunidade. ” ) . Várias abordagens são possíveis e usuais para esse tipo de tarefa , desde a anotação manual das etiquetas e posterior treinamento de modelo , até o uso de redes neurais . As mais conhecidas ainda hoje são as baseadas em regras ( Brill , 1992 ) , em redes neurais artificiais ( Schmid , 1994 ) , as abordagens estocásticas ( Hall , 2003 ) e as híbridas ( Altunyurt ; Orhan ; Güngör , 2006 ; Zin , 2009 ) . A seguir , explicamos brevemente algumas dessas abordagens , sumarizando o trabalho de Zewdu ; Yitagesu ( 2022 ) , que fizeram recentemente uma revisão sistemática da literatura sobre as técnicas usadas para etiquetagem de PoS . Ressaltamos , no', 'summarized_text': 'O texto discute algumas tarefas de processamento de linguagem natural, como a conversão para caracteres minúsculos, a lematização e a radicalização. Também aborda a etiquetagem morfossintática, que envolve a atribuição de etiquetas gramaticais a cada palavra em um texto. São apresentadas diferentes abordagens para essa tarefa, como as baseadas em regras, em redes neurais e as abordagens estocásticas e híbridas.', 'sentence_results': [{'sentence': 'O texto discute algumas tarefas de processamento de linguagem natural, como a conversão para caracteres minúsculos, a lematização e a radicalização.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['processamento de linguagem natural', 'conversão para caracteres minúsculos', 'lematização', 'radicalização']}}, {'sentence': 'Também aborda a etiquetagem morfossintática, que envolve a atribuição de etiquetas gramaticais a cada palavra em um texto.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['etiquetagem morfossintática', 'atribuição de etiquetas gramaticais']}}, {'sentence': 'São apresentadas diferentes abordagens para essa tarefa, como as baseadas em regras, em redes neurais e as abordagens estocásticas e híbridas.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['abordagens', 'regras', 'redes neurais', 'abordagens estocásticas', 'híbridas']}}], 'final_sentiment_count': {'positive': 0, 'negative': 0, 'neutral': 3, 'mixed': 0}, 'SSCORE': 0.0}, {'text': 'entanto , que , para uma visão mais aprofundada , recomendamos ler o artigo original . Após a etapa de PoS tagging para cada uma das palavras de uma sentença , elas passam então para outra etapa do processamento que também pertence à área da Morfologia , que é a atribuição das features ( ou atributos ) morfológicos , e que serão explorados nas próximas seções . A anotação de atributos morfológicos , também conhecida como atribuição de features morfológicas ( ou somente feats ) , é uma importante tarefa que envolve a marcação ou identificação de informações específicas sobre as características gramaticais e morfológicas de palavras em um texto . Esses atributos morfológicos incluem características como número , gênero , modo , tempo , pessoa e outras informações semelhantes . O objetivo principal dessa anotação é capturar e codificar informações gramaticais relevantes de maneira estruturada , tornando possível para algoritmos de PLN e de AM entenderem e processarem corretamente a estrutura e as relações linguísticas presentes em um texto . Por exemplo , considere a palavra “ falávamos ” . A anotação de atributos morfológicos , nesse caso , pode envolver a marcação de tempo ( pretérito imperfeito ) , modo ( indicativo ) , número ( plural ) , pessoa ( primeira ) e forma verbal ( finita ) . No exemplo da Figura 4.2 – “ O gato preto correu rapidamente ” – , a anotação das features morfológicas seria conforme apresentado na Figura 4.3 , gerada pela ferramenta LX-USuite.10 Cabe a cada tagger ( ou etiquetador morfológico ) definir seu próprio conjunto de features ou características morfológicas que devem ser anotadas ou extraídas , o que é chamado de tagset ( ou conjunto de etiquetas ) . Por exemplo , o UDPipe , que será abordado na Seção 4.3.1 , identifica , para verbos , todas as features citadas anteriormente : para o verbo “ comprei ” , lemma = “ comprar ” e features = “ VERB _ Mood=Ind|Number=Sing|Person=1|Tense=Past|VerbForm=Fin ” . A quantidade de etiquetas , bem como seus nomes , pode variar bastante de um tagger para outro , e essa falta de uniformização constitui um grande desafio para o PLN . De acordo com Fonseca ; Rosa ; Aluísio ( 2015 ) , o Penn Treebank do inglês , por exemplo , conta com 45 tags incluindo sinais de pontuação , enquanto o CLAWS5 possui 62 e o CLAWS7 possui 137 tags , sendo que os dois últimos usaram o mesmo corpus ( British National Corpus ) . Se o objetivo da tarefa for treinar um etiquetador morfossintático , a anotação pode ser feita manualmente por especialistas humanos que analisam e selecionam os atributos morfológicos para as palavras em um corpus de treinamento ( Capítulo 14 ) . Posteriormente , modelos de PLN e AM podem ser treinados usando esses dados anotados para automatizar a anotação de novos textos e auxiliar em várias tarefas e aplicações . Por outro lado , já existem também abordagens computacionais semelhantes às mencionadas na Seção 4.2.5 , que também podem ser usadas para etiquetar automaticamente as palavras em relação às suas características morfológicas . Além das tarefas de processamento aqui apresentadas , vale mencionar que há ainda outras , como a análise sintática automática ( tarefa de parsing ) , a segmentação dos constituintes sintáticos dentro da frase ( tarefa de chunking ) , a extração ou anotação de entidades nomeadas ( tarefa de Named Entity Recognition ) etc . O que nos interessa neste Capítulo são as tarefas ligadas ao processamento morfológico e morfossintático , portanto nos limitamos às mencionadas nesta seção . Esta seção apresenta ferramentas computacionais e recursos disponíveis para o português , que processam a língua no nível morfológico e morfossintático . Para cada uma das tarefas de ( pré- ) processamento , existem ferramentas específicas que podem ser usadas para a análise automática do português . Dividimos esta seção em ferramentas ( Seção 4.3.1 ) e recursos ( Seção 4.3.2 ) . Ao final da seção , selecionamos um dos recursos para explicá-lo de forma mais aprofundada , a saber , o PortiLexicon ( Seção 4.3.3 ) . Existem ferramentas que têm como foco tarefas específicas de PLN . Outras , como NLTK11 e spaCy12 , são módulos mais completos que implementam submódulos e funcionalidades diversas . Apresentamos a seguir ferramentas disponíveis para o processamento de textos , separadas por tarefa . O submódulo nltk.stem implementa diferentes abordagens de stemming , incluindo : São chamadas de taggers as ferramentas de etiquetagem morfossintática e morfológica , ou seja , as ferramentas computacionais que atribuem automaticamente o PoS e as etiquetas morfológicas para cada palavra em uma frase . Antes de mencionar os taggers propriamente ditos para o português , convém citar o trabalho de Gonçalves et al . ( 2020 ) , no qual os autores avaliaram várias ferramentas de etiquetagem morfossintática para o português , como : FreeLing21 , NLTK22 , OpenNLP23 , NLPyPort24 , PolyGlot25 , spaCy26 , StanfordNLP27 , TreeTagger28 e LinguaKit29 . Algumas delas fazem apenas o processamento do português europeu , mas a maioria processa as duas variantes do português . Além dos PoS taggers indicados no parágrafo acima , também estão disponíveis para o português : Nesta seção , indicaremos alguns recursos lexicais ( principalmente corpora ) anotados com informação morfológica e morfossintática disponíveis para o português . Não pretendemos aqui explorar todos os recursos disponíveis , mas apenas mencionar alguns exemplos , considerando que o Capítulo 14 aborda corpus e datasets . Ao final da seção , exploraremos de forma aprofundada apenas um deles , a fim de exemplificar e deixar claros todos os conceitos e tarefas apresentados neste capítulo . Os corpora mais amplamente conhecidos na comunidade do português e que contenham anotação de PoS são o Tycho Brahe ( Namiuti , 2004 ) , o Mac-Morpho e o Bosque . O primeiro deles é um corpus do português histórico europeu , que compilou textos', 'summarized_text': 'Nesta seção, é explicado o processo de anotação de atributos morfológicos, que envolve a marcação de informações gramaticais e morfológicas de palavras em um texto. Esses atributos incluem número, gênero, modo, tempo, pessoa e outras características. A anotação é importante para que algoritmos de Processamento de Linguagem Natural (PLN) e Aprendizado de Máquina (AM) possam entender e processar corretamente a estrutura e as relações linguísticas de um texto. A quantidade e os nomes das etiquetas podem variar entre diferentes etiquetadores morfológicos, o que constitui um desafio para o PLN. Existem abordagens computacionais para a anotação automática desses atributos morfológicos. Além disso, são mencionadas outras tarefas de processamento de texto, como análise sintática automática, segmentação de constituintes sintáticos e extração de entidades nomeadas.', 'sentence_results': [{'sentence': 'Nesta seção, é explicado o processo de anotação de atributos morfológicos, que envolve a marcação de informações gramaticais e morfológicas de palavras em um texto.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['anotação', 'atributos morfológicos', 'marcação', 'informações gramaticais', 'morfológicas', 'palavras', 'texto']}}, {'sentence': 'Esses atributos incluem número, gênero, modo, tempo, pessoa e outras características.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['número', 'gênero', 'modo', 'tempo', 'pessoa']}}, {'sentence': 'A anotação é importante para que algoritmos de Processamento de Linguagem Natural (PLN) e Aprendizado de Máquina (AM) possam entender e processar corretamente a estrutura e as relações linguísticas de um texto.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['anotação', 'algoritmos', 'Processamento de Linguagem Natural', 'PLN', 'Aprendizado de Máquina', 'AM', 'estrutura', 'relações linguísticas', 'texto']}}, {'sentence': 'A quantidade e os nomes das etiquetas podem variar entre diferentes etiquetadores morfológicos, o que constitui um desafio para o PLN.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['etiquetas', 'etiquetadores morfológicos', 'desafio', 'PLN']}}, {'sentence': 'Existem abordagens computacionais para a anotação automática desses atributos morfológicos.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['anotação automática', 'atributos morfológicos']}}, {'sentence': 'Além disso, são mencionadas outras tarefas de processamento de texto, como análise sintática automática, segmentação de constituintes sintáticos e extração de entidades nomeadas.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['análise sintática automática', 'segmentação de constituintes sintáticos', 'extração de entidades nomeadas']}}], 'final_sentiment_count': {'positive': 1, 'negative': 0, 'neutral': 5, 'mixed': 0}, 'SSCORE': 16.666666666666664}, {'text': 'literários antigos ( do século XIV ao XIX ) . Por serem de um gênero e domínio muito específicos , além de conter palavras e estruturas arcaicas , ele não é um corpus muito representativo do português contemporâneo . Os outros dois ( Mac-Morpho e Bosque ) serão apresentados na sequência desta seção . Além deles , indicaremos também outros corpora mais recentes do português que contenham anotação morfossintática . Alguns deles serão retomados no Capítulo 7 . Nos repositórios do NILC42 , POeTiSA43 e Linguateca44 você pode ter acesso a vários outros recursos para o português , tais como corpora , léxicos e outros recursos lexicais com anotações morfológica e morfossintática . No Capítulo 14 também serão indicados corpora relevantes para o português , mas que não necessariamente contêm informação morfológica ou morfossintática . Para esclarecer de forma bem detalhada como funcionam esses recursos , a anotação de PoS e de feats , as etiquetas etc. , selecionamos um deles , o PortiLexicon-UD , para ser apresentado em detalhes na próxima seção ( Seção 4.3.3 ) , a fim de exemplificar os conceitos abordados ao longo do capítulo . Para ilustrar o uso , na prática , de um recurso para processamento lexical , esta seção apresenta o PortiLexicon-UD ( Lopes et al. , 2022 ) . O PortiLexicon-UD é um recurso que auxilia na tarefa de identificar as unidades de processamento . Ele é um léxico para o português que elenca palavras e suas anotações morfossintáticas . Especificamente , esse léxico utiliza o padrão Universal Dependencies ( UD ) 45 ( Marneffe et al. , 2021 ) com etiquetas PoS , lema e etiquetas de atributos morfológicos ( gênero , número etc. ) . Na sua versão atual46 , PortiLexicon-UD possui 1.226.339 entradas . O conjunto de etiquetas PoS da UD define 17 classes gramaticais descritas no Quadro 4.1 com alguns exemplos de utilização no português segundo o mapeamento adotado . Todo mapeamento do português para o padrão UD foi feito durante a construção do corpus Porttinari-base ( Duran et al. , 2023 ) . Dessa forma , o mapeamento reflete as decisões tomadas na anotação deste corpus . Quadro 4.1 Etiquetas PoS da UD e sua descrição no português Devido à natureza dos léxicos , as palavras anotadas como pontuações ( PUNCT ) , símbolos ( SYM ) , além de nomes próprios ( PROPN ) e palavras fora do vocabulário ( X ) não estão presentes no PortiLexicon-UD . Levando em conta que a classe PART não é utilizada no mapeamento , o léxico possui palavras pertencentes a 12 classes da UD que são mapeadas no português conforme sumarizado no Quadro 4.2 . Quadro 4.2 Mapeamento da classificação tradicional do português para as etiquetas PoS da UD utilizado no PortiLexicon-UD O PortiLexicon-UD foi construído a partir de um léxico pré-existente , DELAF-PB ( Ranchhod ; Mota ; Baptista , 1999 ) , e suas versões UNITEX-PB ( Muniz , 2004 ) e MorphoBr ( Alencar ; Cuconato ; Rademaker , 2018 ) . Notadamente , foi estendido o vocabulário e diversas situações foram corrigidas e adaptadas conforme descrito no lançamento do léxico ( Lopes et al. , 2022 ) . Todas as entradas no PortiLexicon-UD correspondem a uma tupla47 com quatro informações : Enquanto as palavras das classes ADP , ADV , CCONJ , INTJ e SCONJ usualmente contêm pouca ou nenhuma informação morfológica , outras classes possuem grande variedade de informações . Alguns exemplos de palavras destas cinco classes estão indicadas no Quadro 4.3 . Quadro 4.3 Exemplos de palavras das classes ADP , ADV , CCONJ , INTJ e SCONJ Já as palavras das classes ADJ , NOUN e NUM possuem uma maior variação de atributos morfológicos como pode ser visto no Quadro 4.4 . Usualmente palavras destas classes possuem os atributos Gender ( gênero ) e Number ( número ) , e ocasionalmente os atributos VerbForm ( forma verbal ) e NumType ( tipo numérico ) indicando a origem das palavras . Quadro 4.4 Exemplos de palavras das classes ADJ , NOUN e NUM As palavras etiquetadas como PRON e DET possuem uma variação ainda maior de atributos morfológicos , pois levam em consideração aspectos como o tipo de pronome e o caso . O Quadro 4.5 ilustra alguns destes casos . Quadro 4.5 Exemplos de palavras das classes PRON e DET Finalmente , as palavras que pertencem às classes AUX e VERB também possuem uma variação de atributos morfológicos relevante , porém mais padronizada , já que os atributos descrevem as conjugações dos tempos verbais . O Quadro 4.6 apresenta exemplos dos tempos verbais em português . Quadro 4.6 Exemplos de palavras das classes AUX e VERB Descrito o conteúdo das entradas do PortiLexicon-UD , a Tabela 4.1 apresenta a distribuição das entradas e palavras distintas por etiqueta PoS da UD . Nesta tabela também estão indicadas : ( i ) na coluna palavras , o total de palavras distintas por classe ; ( ii ) na coluna amb , o número de palavras sintaticamente ambíguas , ou seja , palavras que possuem mais do que uma entrada ; ( iii ) na coluna non-amb , o total de palavras não ambíguas e ; finalmente , ( iv ) na coluna entradas , o número total de entradas . É possível perceber que , por serem classes abertas , os verbos , substantivos e adjetivos correspondem à maior parte do léxico . As palavras funcionais , como preposições , conjunções e pronomes , correspondem a uma parte bem menor do léxico , apesar de serem extremamente importantes na linguagem . A maior parte das palavras do PortiLexicon-UD é de palavras não sintaticamente ambíguas e , portanto , possuem uma única entrada . No entanto , algumas palavras são particularmente ambíguas , como é o caso da palavra “ que ” , que possui o maior número de etiquetas PoS associadas a uma palavra do léxico ( 7 ) , podendo ser utilizada', 'summarized_text': \"O PortiLexicon-UD é um recurso léxico para o português que elenca palavras e suas anotações morfossintáticas. Ele utiliza o padrão Universal Dependencies (UD) com etiquetas PoS, lema e etiquetas de atributos morfológicos. O léxico possui 1.226.339 entradas e mapeia palavras em 12 classes gramaticais da UD. As palavras do léxico podem conter informações morfológicas como gênero, número, forma verbal e tipo numérico. As classes ADP, ADV, CCONJ, INTJ e SCONJ geralmente têm pouca informação morfológica, enquanto as classes ADJ, NOUN, NUM, PRON e DET têm maior variação de atributos morfológicos. As classes AUX e VERB possuem uma variação de atributos morfológicos padronizada, relacionada à conjugação dos tempos verbais. O léxico contém principalmente palavras não ambíguas, mas também algumas palavras ambíguas, como 'que'.\", 'sentence_results': [{'sentence': 'O PortiLexicon-UD é um recurso léxico para o português que elenca palavras e suas anotações morfossintáticas.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['PortiLexicon-UD', 'recurso léxico', 'português', 'palavras', 'anotações morfossintáticas']}}, {'sentence': 'Ele utiliza o padrão Universal Dependencies (UD) com etiquetas PoS, lema e etiquetas de atributos morfológicos.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['Universal Dependencies', 'UD', 'etiquetas PoS', 'lema', 'etiquetas de atributos morfológicos']}}, {'sentence': 'O léxico possui 1.226.339 entradas e mapeia palavras em 12 classes gramaticais da UD.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['léxico', 'entradas', 'mapeia', 'palavras', 'classes gramaticais', 'UD']}}, {'sentence': 'As palavras do léxico podem conter informações morfológicas como gênero, número, forma verbal e tipo numérico.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['gênero', 'número', 'forma verbal', 'tipo numérico']}}, {'sentence': 'As classes ADP, ADV, CCONJ, INTJ e SCONJ geralmente têm pouca informação morfológica, enquanto as classes ADJ, NOUN, NUM, PRON e DET têm maior variação de atributos morfológicos.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['ADP', 'ADV', 'CCONJ', 'INTJ', 'SCONJ']}}, {'sentence': 'As classes AUX e VERB possuem uma variação de atributos morfológicos padronizada, relacionada à conjugação dos tempos verbais.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['AUX', 'VERB']}}, {'sentence': \"O léxico contém principalmente palavras não ambíguas, mas também algumas palavras ambíguas, como 'que'.\", 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['ambíguas', 'que']}}], 'final_sentiment_count': {'positive': 1, 'negative': 0, 'neutral': 6, 'mixed': 0}, 'SSCORE': 14.285714285714285}, {'text': 'como ADP , ADV , CCONJ , DET , INTJ , PRON e SCONJ . A palavra que possui o maior número de entradas no léxico é a palavra “ fora ” , que pode ser anotada com 5 etiquetas PoS distintas , mas possui as seguintes onze entradas no PortiLexicon-UD : É importante salientar que o PortiLexicon-UD , por seguir o princípio de anotação de tokens individualmente definido pela UD , não representa palavras compostas , ênclises , mesóclises ou palavras contraídas . Dessa forma , palavras como “ segunda-feira ” , “ contá-lo ” , “ trazê-lo-ia ” ou “ desta ” não fazem parte do léxico . Enquanto palavras compostas e nomes próprios podem ser incluídos em novas versões , as ênclises , mesóclises e palavras contraídas devem ser objeto de um pré-processamento que transforme esse tipo de palavra em seus componentes para buscá-las no léxico , por exemplo : Dessa forma , o PortiLexicon-UD oferece um recurso que permite a identificação de unidades léxicas de processamento de maneira eficiente e precisa . Por exemplo , na anotação de corpus ( seja manual ou automática ) , o léxico pode fornecer um grande suporte à atribuição de etiquetas morfológicas , mas também para a análise de características morfológicas o léxico pode funcionar como uma referência de classificação para cada palavra do português . Este capítulo abordou o processamento automático do português no nível da palavra , que é considerado em PLN como a menor unidade de processamento . Para definir e delimitar essa unidade de processamento ( a palavra ) , no entanto , é necessário considerar as pequenas unidades linguísticas que a constituem , que são os morfemas . Portanto , neste capítulo , trouxemos uma visão geral da linguística e os principais conceitos da morfologia ( morfema , afixo , desinência , radical etc . ) , mas também trouxemos os conceitos da morfossintaxe ( lexema , lexia , léxico , token , type etc . ) , cujo objeto principal de estudo é a classificação das palavras em partes do discurso ( ou PoS ) ( Seção 4.1 ) . As duas áreas , Morfologia e Morfossintaxe , se imbricam e se complementam , já que , para identificar quais são os traços morfológicos de uma palavra , é necessário saber qual o seu PoS . Por outro lado , muitas vezes , para definir o PoS de uma palavra , recorre-se aos seus traços morfológicos . Para traçar um paralelo de forma bem simples entre as áreas e seus objetos de estudo , podemos dizer que , na linguística , a menor unidade significativa da língua é o morfema , que é estudado pela Morfologia . Em PLN , a menor unidade de processamento automático é a palavra , que é estudada pela Morfossintaxe . O Quadro 4.7 apresenta um resumo dessas associações . Quadro 4.7 Resumo dos objetos de estudo das áreas Morfologia e Morfossintaxe Após definir todos esses conceitos ( Seção 4.1 ) relevantes para a linguística e para o Processamento de Linguagem Natural , demonstramos como se faz o processamento morfológico em PLN , indicando as principais tarefas e etapas de processamento dos textos ( Seção 4.2 ) . Por fim , indicamos algumas ferramentas e recursos disponíveis para o português ( Seção 4.3 ) , focando mais especificamente em um deles , a fim de exemplificar a complexidade desse nível de análise e processamento linguístico-computacional . Os recursos apresentados neste capítulo são apenas exemplificativos , mas é importante lembrar que existem vários outros que podem ser mais apropriados para uma aplicação ou outra , dependendo da necessidade e dos objetivos da tarefa . Pretendemos , em uma próxima versão deste capítulo , apresentar atividades e exercícios práticos relacionados à etiquetagem morfológica e morfossintática . Também pretendemos explicar passo a passo como utilizar cada um dos recursos e ferramentas citadas aqui , a fim de contribuir , de forma mais prática e didática , com a internalização dos conceitos e o modus operandi de fazer PLN . Também está prevista para a próxima versão uma apresentação mais global dos tipos de recursos lexicais , como thesaurus ( e.g . o TeP48 ) , redes semânticas ( e.g . WordNetBr49 , VerbNetBr50 , PropBankBr51 ) , léxicos , dicionários comuns , dicionários especializados , ontologias , glossários e tantos outros recursos construídos para o português . Considerando línguas como o português nas quais o processo de tokenização ( apresentado na Seção 4.2 ) pode ser baseado na presença de espaços em branco delimitadores das palavras . Em outras línguas , como o alemão ou o chinês , por exemplo , esse processo de tokenização pode ser um pouco mais complexo.↩︎ Para isso , em muitos modelos , é necessário uma etapa a mais de processamento para isolar o sinal de pontuação da palavra anterior , já que graficamente eles costumam estar juntos.↩︎ https : //www.planalto.gov.br/ccivil_03/leis/l8069.htm↩︎ Fonte : https : //www.biologianet.com/biologia-celular/dna.htm↩︎ https : //www.nltk.org↩︎ https : //portulanclarin.net/workbench/lx-tokenizer/↩︎ Geralmente , um modelo computacional é treinado em um conjunto de dados e usado em outros conjuntos diferentes . Nesse sentido , palavras que nunca foram vistas no conjunto de dados de treinamento são consideradas palavras desconhecidas ( unknown ) nos novos conjuntos . A existência de uma palavra desconhecida faz com que o modelo não saiba como tratá-la , o que pode impactar ( e até mesmo inviabilizar ) o processamento.↩︎ Vale lembrar que o conceito de “ radical ” não inclui a vogal temática . No exemplo de “ trabalho ” , o radical correto seria “ trabalh ” e o “ o ” seria uma vogal temática , portanto a forma “ trabalho ” , na verdade , é o tema ( e não o radical ) , porém essa distinção é muito específica e desnecessária para o PLN . Assim , a tarefa de radicalização , em PLN , pode considerar ora o radical ou a raiz , ora o tema.↩︎', 'summarized_text': \"A palavra 'fora' possui o maior número de entradas no PortiLexicon-UD, sendo anotada com 5 etiquetas PoS distintas e possuindo onze entradas no léxico. O PortiLexicon-UD não representa palavras compostas, ênclises, mesóclises ou palavras contraídas. O léxico pode fornecer suporte à atribuição de etiquetas morfológicas e análise de características morfológicas. A menor unidade significativa da língua é o morfema, estudado pela Morfologia, enquanto em PLN, a menor unidade de processamento automático é a palavra, estudada pela Morfossintaxe. Existem recursos lexicais disponíveis para o português, como thesaurus, redes semânticas, léxicos, dicionários e ontologias. O processo de tokenização pode ser baseado na presença de espaços em branco delimitadores das palavras. Palavras desconhecidas podem impactar o processamento de modelos computacionais. A tarefa de radicalização em PLN pode considerar o radical ou a raiz.\", 'sentence_results': [{'sentence': \"A palavra 'fora' possui o maior número de entradas no PortiLexicon-UD, sendo anotada com 5 etiquetas PoS distintas e possuindo onze entradas no léxico.\", 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['fora']}}, {'sentence': 'O PortiLexicon-UD não representa palavras compostas, ênclises, mesóclises ou palavras contraídas.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['Palavras compostas', 'Ênclises', 'Mesóclises', 'Palavras contraídas']}}, {'sentence': 'O léxico pode fornecer suporte à atribuição de etiquetas morfológicas e análise de características morfológicas.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['léxico', 'atribuição', 'etiquetas morfológicas', 'análise', 'características morfológicas']}}, {'sentence': 'A menor unidade significativa da língua é o morfema, estudado pela Morfologia, enquanto em PLN, a menor unidade de processamento automático é a palavra, estudada pela Morfossintaxe.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['morfema', 'morfologia', 'PLN', 'palavra', 'morfossintaxe']}}, {'sentence': 'Existem recursos lexicais disponíveis para o português, como thesaurus, redes semânticas, léxicos, dicionários e ontologias.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['thesaurus', 'redes semânticas', 'léxicos', 'dicionários', 'ontologias']}}, {'sentence': 'O processo de tokenização pode ser baseado na presença de espaços em branco delimitadores das palavras.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['tokenização', 'espaços em branco', 'delimitadores', 'palavras']}}, {'sentence': 'Palavras desconhecidas podem impactar o processamento de modelos computacionais.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['desconhecidas', 'impactar', 'processamento', 'modelos', 'computacionais']}}, {'sentence': 'A tarefa de radicalização em PLN pode considerar o radical ou a raiz.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['radical', 'raiz']}}], 'final_sentiment_count': {'positive': 2, 'negative': 0, 'neutral': 6, 'mixed': 0}, 'SSCORE': 25.0}, {'text': 'https : //portulanclarin.net/workbench/lx-utagger/↩︎ https : //portulanclarin.net/workbench/lx-usuite/↩︎ https : //www.nltk.org↩︎ https : //spacy.io/↩︎ https : //www.nltk.org/api/nltk.tokenize.html↩︎ https : //spacy.io/api/tokenizer↩︎ https : //spacy.io/api/lemmatizer↩︎ https : //www.inf.ufrgs.br/\\\\~viviane/rslp/index.htm↩︎ https : //snowballstem.org/↩︎ https : //www.nltk.org/howto/portuguese_en.html # sentence-segmentation↩︎ https : //spacy.io/api/sentencizer↩︎ https : //github.com/thalesbertaglia/enelvo ou https : //thalesbertaglia.com/enelvo/sobre/↩︎ https : //nlp.lsi.upc.edu/freeling/index.php/↩︎ https : //www.nltk.org↩︎ https : //opennlp.apache.org/↩︎ https : //github.com/NLP-CISUC/NLPyPort↩︎ https : //draquet.github.io/PolyGlot/↩︎ https : //spacy.io/↩︎ https : //stanfordnlp.github.io/stanfordnlp/↩︎ https : //www.cis.uni-muenchen.de/\\\\~schmid/tools/TreeTagger/↩︎ https : //github.com/citiususc/Linguakit↩︎ https : //universaldependencies.org↩︎ https : //github.com/huggingface/transformers↩︎ https : //huggingface.co/spaces/Emanuel/porttagger↩︎ https : //cran.r-project.org/web/packages/udpipe/vignettes/udpipe-train.html↩︎ https : //ufal.mff.cuni.cz/morphodita↩︎ https : //stanfordnlp.github.io/stanza/pos.html↩︎ https : //spacy.io/api/tagger↩︎ http : //nilc.icmc.usp.br/macmorpho/↩︎ Dados da última versão ( v3 ) , disponível em http : //nilc.icmc.usp.br/macmorpho/.↩︎ https : //www.linguateca.pt/Floresta/↩︎ https : //sites.google.com/icmc.usp.br/poetisa/resources-and-tools↩︎ https : //github.com/LR-POR/MorphoBr↩︎ https : //sites.google.com/view/nilc-usp/resources-and-tools ? authuser=0↩︎ https : //sites.google.com/icmc.usp.br/poetisa/the-project↩︎ https : //www.linguateca.pt↩︎ Para um estudo mais aprofundado do projeto Universal Dependencies , ver Capítulo 6.↩︎ O PortiLexicon-UD está disponível no endereço : https : //portilexicon.icmc.usp.br/ e lá também pode ter seus arquivos de dados baixados.↩︎ Tupla é um termo bem usado em computação , e significa uma cadeia de dois ou mais itens . Para um conjunto ordenado de elementos que são números , usamos o termo “ vetor ” . Mas , quando temos um conjunto ordenado no qual os elementos são outras coisas , como a cadeia de informações sobre uma dada palavra ou entrada de de um dicionário , dizemos “ tupla ” .↩︎ http : //www.nilc.icmc.usp.br/tep2/↩︎ http : //www.nilc.icmc.usp.br/wordnetbr/↩︎ http : //143.107.183.175:21380/portlex/index.php/en/projects/verbnetbringl↩︎ http : //143.107.183.175:21380/portlex/index.php/en/projects/propbankbringl↩︎', 'summarized_text': 'Here is a list of various tools and resources for natural language processing:\\n\\n1. LX-Tagger and LX-Suite from PortulanCLARIN: Tools for part-of-speech tagging and lemmatization.\\n2. NLTK (Natural Language Toolkit): A Python library for NLP tasks.\\n3. SpaCy: An industrial-strength NLP library with various features.\\n4. RSLP Stemmer and Snowball Stemmer: Stemming algorithms for the Portuguese language.\\n5. Sentence segmentation resources: NLTK and SpaCy provide tools for sentence segmentation.\\n6. Enelvo: A tool for Portuguese language normalization and expansion.\\n7. Freeling: A suite of language analyzers for several languages including Portuguese.\\n8. OpenNLP: A machine learning-based toolkit for NLP tasks.\\n9. NLPyPort: A library for Portuguese text processing.\\n10. PolyGlot: A multilingual NLP library with support for Portuguese.\\n11. StanfordNLP: A suite of NLP tools developed by Stanford University.\\n12. TreeTagger: A tool for part-of-speech tagging developed by the University of Munich.\\n13. Linguakit: A library for various NLP tasks in Portuguese.\\n14. Universal Dependencies: A framework for annotating and parsing natural language corpora.\\n15. Hugging Face Transformers: A library for natural language understanding using pre-trained models.\\n16. PortTagger from Hugging Face: A tool for part-of-speech tagging in Portuguese.\\n17. UDPipe: A trainable pipeline for various NLP tasks including part-of-speech tagging.\\n18. MorphoDiTa: A morphological dictionary and tagger for the Czech language.\\n19. StanfordNLP: A suite of NLP tools developed by Stanford University, including part-of-speech tagging.\\n20. Mac-Morpho Corpus: A corpus of Brazilian Portuguese text.\\n21. Floresta Sintá(c)tica: A syntactic treebank for Portuguese.\\n22. POETISA: A semantic role labeling framework for Portuguese.\\n23. MorphoBr: A morphological analyzer and generator for Brazilian Portuguese.\\n24. NILC: A research group at the University of São Paulo that develops various NLP resources and tools.\\n25. Linguateca: A website that provides various resources and tools for Portuguese NLP.\\n26. Universal Dependencies Project: A project that aims to develop cross-linguistically consistent treebank annotation for many languages.\\n27. PortiLexicon-UD: A lexicon for Portuguese based on Universal Dependencies.\\n28. Tupla: A term used in computer science to refer to a sequence of two or more items.\\n29. TEP2: A platform for collaborative development of Portuguese language resources.\\n30. WordNet-BR: A Brazilian Portuguese version of WordNet.\\n31. VERBNET-BR and PropBank-BR: Projects that adapt the VerbNet and PropBank resources for Brazilian Portuguese.', 'sentence_results': [{'sentence': 'Here is a list of various tools and resources for natural language processing:\\n\\n1.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['NLTK', 'spaCy', 'Word2Vec', 'Gensim', 'BERT', 'Transformer', 'Stanford NLP', 'CoreNLP', 'TextBlob', 'FastText', 'OpenNLP', 'AllenNLP', 'Flair', 'Hugging Face', 'PyTorch', 'TensorFlow', 'scikit-learn', 'PySpark', 'Natural Language Understanding (NLU)', 'Natural Language Generation (NLG)']}}, {'sentence': 'LX-Tagger and LX-Suite from PortulanCLARIN: Tools for part-of-speech tagging and lemmatization.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['LX-Tagger', 'LX-Suite', 'part-of-speech tagging', 'lemmatization']}}, {'sentence': '2.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['cat', 'dog', 'bird']}}, {'sentence': 'NLTK (Natural Language Toolkit): A Python library for NLP tasks.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['NLTK', 'Natural Language Toolkit', 'Python', 'library', 'NLP']}}, {'sentence': '3.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['apple', 'banana', 'orange']}}, {'sentence': 'SpaCy: An industrial-strength NLP library with various features.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['SpaCy', 'NLP', 'library', 'industrial-strength', 'features']}}, {'sentence': '4.', 'print_sentiment': {'sentiment': 'mixed'}, 'print_keywords': {'keywords': ['apple', 'banana', 'orange']}}, {'sentence': 'RSLP Stemmer and Snowball Stemmer: Stemming algorithms for the Portuguese language.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['RSLP Stemmer', 'Snowball Stemmer', 'Portuguese language']}}, {'sentence': '5.', 'print_sentiment': {'sentiment': 'mixed'}, 'print_keywords': {'keywords': ['apple', 'banana', 'orange']}}, {'sentence': 'Sentence segmentation resources: NLTK and SpaCy provide tools for sentence segmentation.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['NLTK', 'SpaCy', 'sentence segmentation', 'tools']}}, {'sentence': '6.', 'print_sentiment': {'sentiment': 'mixed'}, 'print_keywords': {'keywords': []}}, {'sentence': 'Enelvo: A tool for Portuguese language normalization and expansion.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['Enelvo', 'Portuguese language', 'normalization', 'expansion']}}, {'sentence': '7.', 'print_sentiment': {'sentiment': 'mixed'}, 'print_keywords': {'keywords': ['function', 'print', 'sentiment', 'keywords']}}, {'sentence': 'Freeling: A suite of language analyzers for several languages including Portuguese.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['Freeling', 'language analyzers', 'Portuguese']}}, {'sentence': '8.', 'print_sentiment': {'sentiment': 'mixed'}, 'print_keywords': {'keywords': []}}, {'sentence': 'OpenNLP: A machine learning-based toolkit for NLP tasks.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['OpenNLP', 'machine learning', 'NLP', 'natural language processing', 'toolkit']}}, {'sentence': '9.', 'print_sentiment': {'sentiment': 'negative'}, 'print_keywords': {'keywords': ['AI', 'machine learning', 'data science']}}, {'sentence': 'NLPyPort: A library for Portuguese text processing.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['NLP', 'Portuguese', 'text processing']}}, {'sentence': '10.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['AI', 'Machine Learning', 'Data Science']}}, {'sentence': 'PolyGlot: A multilingual NLP library with support for Portuguese.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['NLP library', 'multilingual', 'Portuguese']}}, {'sentence': '11.', 'print_sentiment': {'sentiment': 'mixed'}, 'print_keywords': {'keywords': ['chatbot', 'AI', 'machine learning']}}, {'sentence': 'StanfordNLP: A suite of NLP tools developed by Stanford University.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['NLP', 'tools', 'Stanford University']}}, {'sentence': '12.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['12']}}, {'sentence': 'TreeTagger: A tool for part-of-speech tagging developed by the University of Munich.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['TreeTagger', 'part-of-speech tagging', 'University of Munich']}}, {'sentence': '13.', 'print_sentiment': {'sentiment': 'mixed'}, 'print_keywords': {'keywords': ['13']}}, {'sentence': 'Linguakit: A library for various NLP tasks in Portuguese.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['Linguakit', 'NLP', 'Portuguese', 'library']}}, {'sentence': '14.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['machine learning', 'data science', 'artificial intelligence']}}, {'sentence': 'Universal Dependencies: A framework for annotating and parsing natural language corpora.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['Universal Dependencies', 'annotation', 'parsing']}}, {'sentence': '15.', 'print_sentiment': {'sentiment': 'mixed'}, 'print_keywords': {'keywords': ['15']}}, {'sentence': 'Hugging Face Transformers: A library for natural language understanding using pre-trained models.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['Hugging Face', 'Transformers', 'library', 'natural language understanding', 'pre-trained models']}}, {'sentence': '16.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['16']}}, {'sentence': 'PortTagger from Hugging Face: A tool for part-of-speech tagging in Portuguese.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['PortTagger', 'Hugging Face', 'part-of-speech tagging', 'Portuguese']}}, {'sentence': '17.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['17']}}, {'sentence': 'UDPipe: A trainable pipeline for various NLP tasks including part-of-speech tagging.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['UDPipe', 'trainable', 'pipeline', 'NLP', 'part-of-speech tagging']}}, {'sentence': '18.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['18']}}, {'sentence': 'MorphoDiTa: A morphological dictionary and tagger for the Czech language.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['MorphoDiTa', 'morphological dictionary', 'tagger', 'Czech language']}}, {'sentence': '19.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['19']}}, {'sentence': 'StanfordNLP: A suite of NLP tools developed by Stanford University, including part-of-speech tagging.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['StanfordNLP', 'NLP', 'tools', 'part-of-speech tagging']}}, {'sentence': '20.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['20']}}, {'sentence': 'Mac-Morpho Corpus: A corpus of Brazilian Portuguese text.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['Mac-Morpho Corpus', 'Brazilian Portuguese', 'text']}}, {'sentence': '21.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['21']}}, {'sentence': 'Floresta Sintá(c)tica: A syntactic treebank for Portuguese.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['Floresta Sintá(c)tica', 'syntactic treebank', 'Portuguese']}}, {'sentence': '22.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['22', 'twenty two', 'number']}}, {'sentence': 'POETISA: A semantic role labeling framework for Portuguese.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['POETISA', 'semantic role labeling', 'framework', 'Portuguese']}}, {'sentence': '23.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['23']}}, {'sentence': 'MorphoBr: A morphological analyzer and generator for Brazilian Portuguese.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['morphological analyzer', 'generator', 'Brazilian Portuguese']}}, {'sentence': '24.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['24']}}, {'sentence': 'NILC: A research group at the University of São Paulo that develops various NLP resources and tools.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['NLP', 'research group', 'University of São Paulo']}}, {'sentence': '25.', 'print_sentiment': {'sentiment': 'mixed'}, 'print_keywords': {'keywords': ['25']}}, {'sentence': 'Linguateca: A website that provides various resources and tools for Portuguese NLP.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['Linguateca', 'website', 'resources', 'tools', 'Portuguese', 'NLP']}}, {'sentence': '26.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['26']}}, {'sentence': 'Universal Dependencies Project: A project that aims to develop cross-linguistically consistent treebank annotation for many languages.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['Universal Dependencies Project', 'cross-linguistic', 'treebank annotation', 'languages']}}, {'sentence': '27.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['27']}}, {'sentence': 'PortiLexicon-UD: A lexicon for Portuguese based on Universal Dependencies.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['lexicon', 'Portuguese', 'Universal Dependencies']}}, {'sentence': '28.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['28']}}, {'sentence': 'Tupla: A term used in computer science to refer to a sequence of two or more items.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['Tupla', 'computer science', 'sequence', 'items']}}, {'sentence': '29.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['29']}}, {'sentence': 'TEP2: A platform for collaborative development of Portuguese language resources.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['TEP2', 'collaborative development', 'Portuguese language resources']}}, {'sentence': '30.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['30']}}, {'sentence': 'WordNet-BR: A Brazilian Portuguese version of WordNet.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['WordNet-BR', 'Brazilian Portuguese', 'version']}}, {'sentence': '31.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['31']}}, {'sentence': 'VERBNET-BR and PropBank-BR: Projects that adapt the VerbNet and PropBank resources for Brazilian Portuguese.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['VerbNet', 'PropBank', 'Brazilian Portuguese']}}], 'final_sentiment_count': {'positive': 28, 'negative': 1, 'neutral': 24, 'mixed': 9}, 'SSCORE': 43.54838709677419}], 'Capítulo 22: PLN no Direito': [{'text': 'Perspectivas e desafios com textos jurídicos e legais Maria José Bocorny Finatto Aline Macohin 26/09/2023 PDF Neste capítulo , tratamos de diferentes aspectos associados ao trabalho computacional com textos produzidos na esfera do Direito . As tarefas de PLN envolvidas , em geral , são a análise textual e a representação de conteúdos por meio de diferentes técnicas , mas há várias abordagens e estudos , voltados para diferentes finalidades . O nosso objetivo é apresentar apenas algumas perspectivas e desafios no âmbito de trabalhos que exploram materiais produzidos em português , considerando somente o cenário do Direito Brasileiro . Afinal , o Direito , de país para país , tem especificidades linguísticas e culturais que repercutem muito sobre seus textos , discursos e tipo de vocabulário . Por isso , iniciamos o capítulo apresentando alguns aspectos sócio-históricos do Direito Brasileiro , que acabam influenciando suas práticas de escrita e os seus conteúdos textuais . Em seguida , situamos exemplos de reconhecimento e exploração do vocabulário jurídico , dos seus modos de dizer e , especialmente , das suas terminologias . Vamos partir de dois diferentes cenários textuais : as leis e sentenças judiciais . A primeira parte de exemplos tem a ver com um trabalho que denominamos reconhecimento terminológico ( RT ) . Esse trabalho , atualmente , é baseado em fontes escritas disponíveis em formato digital e se beneficia muito das técnicas da Linguística de Corpus ( Sardinha , 2000 ) e do PLN . Depois dessa parte , mais dedicada ao vocabulário e terminologias , segue um exemplo de estudo em PLN , na área conhecida como Análise de Sentimentos ( Capítulo 23 ) . O território de materiais para estudo e de enfoques , em Direito , é extremamente amplo , isso se ficarmos restritos aos trabalhos que lidam com os textos jurídicos da atualidade . Ainda assim , vale mencionar que uma série de estudos históricos sobre a linguagem jurídica brasileira , tratando de seus conceitos e até preconceitos , têm sido muito úteis para uma crítica social e política sobre o Direito . Para esses estudos históricos , os processos sobre crimes no período colonial e do império , reunidos em corpora que se exploram com apoio computacional , têm mostrado a importância de se fazer uma linha de tempo de ações e de entendimentos até os dias de hoje . No Brasil , temos já , por exemplo , diferentes pesquisas filológicas e linguísticas dedicadas a estudos de processos criminais dos séculos 17 , 18 e 19 . Entretanto , para se trabalhar com textos antigos em português , há todo o processo de normalizar e padronizar a apresentação escrita das “ palavras antigas ” , para então podermos fazer o seu processamento . A normalização de textos é , assim , um desafio multidisciplinar de uma nova área de estudos denominada Humanidades Digitais e que inclui o PLN no tratamento de acervos antigos . Conforme nos coloca o artigo de Cameron ; Olival ; Vieira ( 2023 ) , os desafios são muitos . Afinal , geralmente trabalha-se com textos em forma de arquivo provenientes de manuscritos que foram “ decifrados ” e transcritos . Isso significa enfrentar muitas questões associadas à variabilidade da escrita . Afinal , uma mesma palavra podia apresentar-se de vários modos , em um mesmo documento , escrito por uma mesma pessoa , como nos casos de ÁGUA/AGUA/AGOA ou UMA/HUA/HUMA . Há exemplos interessantes desses tipo de estudo histórico , no âmbito do Direito Penal e da Medicina Legal , com processos judiciais que envolveram crimes contra mulheres no Brasil do século 19 . O artigo de Teixeira ; Marengo ; Finatto ( 2022 ) ilustra um exemplo de estudo bem interessante nesse tema da violência contra as mulheres . Mas , voltando à atualidade dos textos e da linguagem do Direito , veremos , mais adiante , como exemplos , alguns textos jurídicos brasileiros , buscando ilustrar suas peculiaridades . Vamos destacar : a ) o texto do Estatuto da Criança e do Adolescente ( ECA ) , conforme apresentado na Lei 8.069-90 , promulgada em 13 de julho de 1990 e atualizada em 2021 ; b ) o texto da nossa Constituição do Brasil , de 1988 ( CF88 ) ; e , c ) um conjunto de Sentenças Judiciais dos chamados “ tribunais de pequenas causas ” , os Juizados Especiais Cíveis . Conforme pretendemos deixar claro , esses três tipos de fontes , em suas características linguísticas e textuais , podem estar associados a diferentes tarefas de PLN , desde a descrição do português até a pontos específicos de Recuperação da Informação , área conhecida como Information Retrieval ( Capítulo 16 ) . Por isso , um outro exemplo que trazemos neste capítulo é o da análise de conteúdos em sentenças judiciais via Análise de Sentimentos . Este tipo de técnica pode ser muito útil para identificar , por exemplo , padrões de sentenças judiciais favoráveis ou desfavoráveis a um determinado assunto . A utilidade dessa técnica para os profissionais do Direito é grande , pois um profissional geralmente faz buscas para entender como um determinado tribunal já vem decidindo sobre um assunto específico . Nesse trabalho de pesquisa , são buscadas retornadas inúmeras sentenças e documentos . Vale destacar que existem tribunais no Brasil inteiro , que lidam diversos assuntos ( trabalhistas , penais , civis , eleitorais , entre outros ) . Nesses órgãos são protocolados milhares de novos processos diariamente e neles existe uma base de milhões de processos já julgados , muitos já em formato digital . A criação de um método que possa filtrar , por exemplo , as causas que foram consideradas favoráveis , em um dado tema , tende a reduzir o trabalho de leitura individual de cada sentença , ajudando o profissional a buscar e encontrar a informação que precisa . Conforme já mostraram os estudos ( Motta , 2021 , 2022 ) , o', 'summarized_text': 'Neste capítulo, são abordadas diversas perspectivas e desafios relacionados ao trabalho computacional com textos jurídicos e legais. O objetivo é apresentar algumas abordagens e estudos no contexto do Direito Brasileiro. São discutidos aspectos sócio-históricos do Direito no Brasil, o reconhecimento e exploração do vocabulário jurídico, a análise de sentimentos em textos jurídicos e o uso de técnicas de Recuperação da Informação. O capítulo também menciona a importância de estudos históricos sobre a linguagem jurídica brasileira e a normalização de textos antigos para o processamento computacional. A análise de conteúdos em sentenças judiciais via Análise de Sentimentos é destacada como uma técnica útil para identificar padrões de decisões judiciais. A criação de métodos que possam filtrar e identificar causas favoráveis em um dado tema pode ajudar os profissionais do Direito a encontrar a informação que precisam.', 'sentence_results': [{'sentence': 'Neste capítulo, são abordadas diversas perspectivas e desafios relacionados ao trabalho computacional com textos jurídicos e legais.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['trabalho computacional', 'textos jurídicos', 'desafios', 'legais']}}, {'sentence': 'O objetivo é apresentar algumas abordagens e estudos no contexto do Direito Brasileiro.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['abordagens', 'estudos', 'Direito Brasileiro']}}, {'sentence': 'São discutidos aspectos sócio-históricos do Direito no Brasil, o reconhecimento e exploração do vocabulário jurídico, a análise de sentimentos em textos jurídicos e o uso de técnicas de Recuperação da Informação.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['sócio-históricos do Direito no Brasil', 'reconhecimento e exploração do vocabulário jurídico', 'análise de sentimentos em textos jurídicos', 'técnicas de Recuperação da Informação']}}, {'sentence': 'O capítulo também menciona a importância de estudos históricos sobre a linguagem jurídica brasileira e a normalização de textos antigos para o processamento computacional.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['estudos históricos', 'linguagem jurídica brasileira', 'normalização de textos antigos', 'processamento computacional']}}, {'sentence': 'A análise de conteúdos em sentenças judiciais via Análise de Sentimentos é destacada como uma técnica útil para identificar padrões de decisões judiciais.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['análise de conteúdos', 'sentenças judiciais', 'análise de sentimentos', 'padrões de decisões judiciais']}}, {'sentence': 'A criação de métodos que possam filtrar e identificar causas favoráveis em um dado tema pode ajudar os profissionais do Direito a encontrar a informação que precisam.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['filtrar', 'identificar', 'causas favoráveis', 'profissionais do Direito']}}], 'final_sentiment_count': {'positive': 2, 'negative': 0, 'neutral': 4, 'mixed': 0}, 'SSCORE': 33.33333333333333}, {'text': 'Direito se manifesta através da língua , pois são as palavras que emprega e os enunciados que produz que conferem e confirmam a sua existência peculiar ( Maciel , 2001 ) como uma prática social e área de conhecimento . Assim , temos uma relação intensa entre o Direito e a língua em uso pelas pessoas que nele atuam . Isto é , pelo emprego de certas palavras1 , com um sentido particular e pela forma como suas proposições e teses são enunciadas vemos todo um cenário de valores . Isso é tão importante que temos uma área de estudos específica conhecida como jurilinguística ( veja mais em Cornu ( 1990 ) ) . Estudiosos dessa área da linguística ( Montoro , 1998 , p. 1998 ) explicam que a linguagem jurídica , sempre com destaque para escrita , compreende diversas “ espécies ” de práticas que se subdividem , conforme uma dada finalidade e foco . Vejamos um detalhamento dessas espécies ou modos de se apresentar conforme seus propósitos ( Petri , 2017 , p. 47 ) : Assim , embora se possa pensar numa linguagem jurídica em geral , quando lidamos com sentenças produzidas em processos judiciais , temos linguagem judiciária , forense ou processual . Quando lidamos com os textos de leis , decretos e portarias , temos a linguagem legislativa . Cada tipo de suporte e/ou instrumento jurídico tende a adotar usos diferenciados e um vocabulário diferenciado . E esses elementos podem ser importantes quando se trabalha com o processamento em larga ou pequena escala desses textos . Como há uma especificidade de discursos envolvida , considerar os seus elementos linguísticos e modos de dizer próprios poderá nos ajudar a desempenhar tarefas de um modo mais produtivo . Afinal , o “ Direito é , por excelência , entre as que mais o sejam , a ciência da palavra . Mais precisamente : do uso dinâmico da palavra ” ( Xavier , 2002 , p. 1 ) . Conforme já mencionamos no início deste capítulo , em cada país , a linguagem jurídica tende a realizar um uso particular da língua comum . Por isso , a linguagem do Direito de um país se diferencia da de um outro – como acontece com a linguagem jurídica dos diferentes países de Língua Portuguesa . Embora haja elos comuns , o Direito brasileiro é bastante distinto do de países como Angola ou Portugal . E , mesmo os textos jurídicos , em suas diversas formas ( no Brasil conhecidos como petições , recursos , decisões judiciais etc . ) podem adotar nomes e modelos de apresentação diversos . Conforme a cultura jurídica e as tradições de cada país , os produtores dos textos jurídicos serão também “ autorias ” diferentes , conforme o que é estabelecido no ordenamento legal de cada país . O Direito no Brasil é regido pelo sistema da civil law , isso significa que uma a lei escrita tem preponderância sobre a jurisprudência – que são as decisões dos juízes – lembrando que os juízes são encarregados de verificar e direcionar a aplicação das leis . No Brasil , quem produz as leis são os membros do poder legislativo , eleitos , democraticamente , pelo povo . Os textos das leis são discutidos e votados , e então aprovados para entrarem em vigor . Os membros do poder executivo , também eleitos pelo povo , devem executar as leis aprovadas . Vejamos um resumo sobre como se organiza o Direito brasileiro , atualmente , em suas hierarquias : Apesar de o sistema jurídico brasileiro ser o civil law , há grande influência da jurisprudência nas decisões judiciais , principalmente quando agrupadas pelos tribunais e transformadas em súmulas . A súmula é um tipo de documento que consiste em um verbete que registra a interpretação pacífica ou majoritária adotada por um Tribunal a respeito de um tema específico . Portanto , quando textos legislativos e documentos processuais tornam-se objetos do PLN , com vistas a obter conhecimento para os profissionais do Direito , será preciso compreender esses elementos e valores diferenciados . Sem isso , há o risco de “ misturar alhos com bugalhos ” . Grosso modo , um RT equivale à identificação e à sistematização de denominações associadas a conceitos conforme utilizadas em um dado campo ou área do conhecimento . Geralmente , o RT envolve a produção de uma “ lista ” de nomes ( termos ) vinculados aos seus significados ( conceitos ) . Além disso , junto de cada item dessa “ lista ” , tem-se um conjunto de informações que ajudam a contextualizar e a entender o seu uso ao longo de um conjunto de documentos escritos . Assim , vamos pensar nesse processo ao longo de um conjunto de documentos jurídicos - em um dado tipo - tendo em mente a situação particular do uso de suas palavras . A Terminologia e os terminólogos dedicam-se a estudar – descrever e compreender - os diferentes fenômenos linguísticos da comunicação técnico-científica , o que se estende ao Direito , em seus variados cenários . O que diferencia uma terminologia de uma palavra “ comum ” é , em primeiro plano , o seu ambiente comunicativo . E , repetindo a ideia de uma das maiores autoridades da nossa área da Terminologia ( Cabré , 2005 ) , podemos dizer : uma palavra não é um termo técnico-científico , ela está nessa condição em determinados contextos , que conferem a ela um significado “ especial ” . Esse significado ou modo de compreensão especial , chamaremos , grosso modo , de conceito . Vejamos um exemplo , com a palavra/item CRIANÇA , muito corriqueira no nosso dia a dia . Como seu significado básico , geralmente , entendemos algo como “ pessoa não adulta ” . Mas , quando empregada e “ significada ” em um dado ambiente comunicativo de especialidade , como é o caso do Direito brasileiro , essa', 'summarized_text': 'O direito se manifesta através da língua, sendo as palavras e enunciados que conferem e confirmam sua existência peculiar como uma prática social e área de conhecimento. Existe uma relação intensa entre o direito e a língua, com diferentes tipos de linguagem jurídica dependendo do contexto e propósito. No Brasil, o direito é regido pelo sistema da civil law, onde a lei tem preponderância sobre a jurisprudência. A jurisprudência, entretanto, possui influência nas decisões judiciais. Quando trabalhando com textos jurídicos, é importante compreender os elementos e valores diferenciados, para evitar misturar conceitos. A terminologia jurídica estuda os diferentes fenômenos linguísticos na comunicação técnico-científica do direito. Uma palavra se torna um termo técnico-científico em determinados contextos, conferindo a ela um significado especial, chamado de conceito.', 'sentence_results': [{'sentence': 'O direito se manifesta através da língua, sendo as palavras e enunciados que conferem e confirmam sua existência peculiar como uma prática social e área de conhecimento.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['direito', 'manifesta', 'língua', 'palavras', 'enunciados', 'prática social', 'área de conhecimento']}}, {'sentence': 'Existe uma relação intensa entre o direito e a língua, com diferentes tipos de linguagem jurídica dependendo do contexto e propósito.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['direito', 'língua', 'linguagem jurídica']}}, {'sentence': 'No Brasil, o direito é regido pelo sistema da civil law, onde a lei tem preponderância sobre a jurisprudência.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['Brasil', 'direito', 'sistema', 'civil law', 'lei', 'jurisprudência']}}, {'sentence': 'A jurisprudência, entretanto, possui influência nas decisões judiciais.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['jurisprudência', 'influência', 'decisões judiciais']}}, {'sentence': 'Quando trabalhando com textos jurídicos, é importante compreender os elementos e valores diferenciados, para evitar misturar conceitos.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['elementos', 'valores', 'diferenciados', 'conceitos', 'textos jurídicos']}}, {'sentence': 'A terminologia jurídica estuda os diferentes fenômenos linguísticos na comunicação técnico-científica do direito.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['terminologia jurídica', 'comunicação técnico-científica', 'direito']}}, {'sentence': 'Uma palavra se torna um termo técnico-científico em determinados contextos, conferindo a ela um significado especial, chamado de conceito.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['palavra', 'termo técnico-científico', 'contextos', 'significado especial', 'conceito']}}], 'final_sentiment_count': {'positive': 3, 'negative': 0, 'neutral': 4, 'mixed': 0}, 'SSCORE': 42.857142857142854}, {'text': 'palavra “ comum ” assume contornos semânticos diferenciados . No contexto do nosso Estatuto da Criança e do Adolescente , documento brasileiro conhecido como ECA , que corresponde à Lei 8.069-90 , atualizada em 2021 , que podemos enquadrar no domínio do Direito Civil do Brasil , temos o seguinte : “ Art . 2º Considera-se criança , para os efeitos desta Lei , a pessoa até doze anos de idade incompletos , e adolescente aquela entre doze e dezoito anos de idade ” ( BRASIL , 1990 , grifo nosso ) . Como se percebe , há um significado “ especializado ” , jurídico , uma delimitação em termos de anos de idade , que se soma ao nosso entendimento mais comum de criança . E você deve estar se perguntando : o que isso importará ou pode repercutir em um trabalho computacional sobre o tema das crianças em leis e documentos em português ? A resposta é : importa muito ! Se comparar com os que estabelece a OMS , Organização Mundial da Saúde , a faixa etária de uma pessoa considerada como criança é outra , pois compreende pessoas até 19 anos de idade . Isto é , os traços/valores de uso da palavra , que adquire estatuto terminológico , são variáveis . Além disso , temos uma conceituação jurídica específica/particular associada a um dado termo que , à primeira vista , não pareceria ser um termo . No caso do segmento de lei acima , o ECA , podemos considerar que há uma definição específica para CRIANÇA , que se opõe à de ADOLESCENTE . Além disso , essa definição é circunscrita , isto é , ela vale apenas em um dado contexto ou “ frame de significação ” . Assim , teríamos um problema , para aquelas pessoas que se interessassem pelo Direito das Crianças , seja em sistemas jurídicos específicos , como o do Brasil , ou que busquem um mapeamento sobre esse tema no âmbito do Direito Internacional , não é mesmo ? Vamos supor uma aplicação de PLN que pudesse nos ajudar a dar conta de uma busca de informações sistematizada sobre esse tema , mas restrita ao cenário brasileiro . Como vimos , em Direito , temos uma definição que tende a ser circunscrita , isto é , ela vale apenas em um dado contexto , correspondendo a um valor que estabelece frente a todo um CONJUNTO DE OUTROS TERMOS E CONCEITOS com ela relacionados . Isso é o que chamamos de sistema conceitual , que tem a ver como uma rede de conceitos e terminologias que se entrelaçam . Como vimos , o ECA está subordinado à Constituição do Brasil , e ainda podemos ter , por exemplo , leis estaduais ou municipais – ou mesmo códigos e portarias – que “ valem como leis locais ” sobre o tratamento de crianças em estabelecimentos de Saúde em diferentes estados do Brasil . Além das normas , também podem haver interpretações jurídicas unânimes ou diversas sobre assuntos relacionados à criança e disponibilizadas em sentenças judiciais . Para realizar um ensaio de um RT , podemos explorar um conjunto de textos que servem de referência ou espelhamento em uma dada área de conhecimento ( veja um passo a passo detalhado com a Constituição do Brasil em Finatto ; Esteves ; Villar ( 2022 ) . Lidando com textos jurídicos , como vimos , será importante levar em conta suas naturezas e tipologias . Vamos supor que um RT associado , por hipótese ao tema “ Direitos das Crianças no Brasil ” . Esse RT poderia envolver identificar , em diferentes documentos relevantes previamente selecionados , os seguintes elementos : a ) TERMOS e seus respectivos CONCEITOS b ) TERMOS e seus respectivos FORMATOS LINGUÍSTICOS c ) TERMOS , CONCEITOS e respectivos TERMOS E CONCEITOS RELACIONADOS . Nos itens a ) e b ) , acima , entra em jogo uma questão muito importante : a variação terminológica . Essa variação tem a ver com as diferentes formas das denominações , dentro de uma dada especialidade ou subárea , que um TERMO pode ter . Você poderá perguntar : vamos explorar esse tema nos âmbito do Direito Civil até o Direito Criminal ? Ou vamos ficar apenas em um dado recorte ? Para administrar a variabilidade de termos e conceitos , sem a ideia de condená-la , pois o enfoque linguístico e conceitual em um RT é sempre descritivo , temos , para nos socorrer , os vocabulários controlados e/ou padronizados . Esses vocabulários mostram padrões de denominações que geralmente são colocados pela autoridade de órgãos profissionais associados a uma dada especialidade . Nesses vocabulários , encontramos as “ terminologias padronizadas ” e também as “ normas técnicas ” de uma área . Assim , uma forma de denominar um respectivo conceito/significado é estabelecida em um dado contexto , de modo a se garantir precisão e boa correlação com outros termos e conceitos relacionados . Isso será importante especialmente em situações de trocas de conhecimento e de trocas em geral . Guardadas as devidas diferenças , é semelhante o caso , por exemplo , do conceito de CRIANÇA frente ao conceito de ADOLESCENTE no nosso Estatuto da Criança e do Adolescente , o ECA . Crianças não poderão ser confundidas , em um cenário legal e jurídico , com adolescentes ou pessoas adultas , salvo condições especiais definidas naquele texto , que funciona como uma moldura de significação para suas terminologias . O mesmo vemos nos casos dos nomes “ oficiais ” para algumas doenças , que inclusive correspondem a um código numérico , conhecido como CID ou Classificação Internacional de Doenças . A ideia , nesse contexto de padronização das terminologias da área da Saúde , é evitar confusões e tentar garantir que todos possam ter um mesmo entendimento – ou conceito uniforme – de um dado TERMO + CONCEITO/DESCRIÇÃO DE SEU SIGNIFICADO . Abaixo , alguns exemplos dessa padronização da CID', 'summarized_text': \"A palavra 'comum' assume diferentes significados no contexto do Estatuto da Criança e do Adolescente (ECA), que define criança como pessoa até 12 anos incompletos e adolescente como pessoa entre 12 e 18 anos. Isso mostra a importância de considerar o contexto jurídico ao estudar o tema das crianças em leis e documentos em português. Além disso, o ECA está subordinado à Constituição do Brasil e existem leis locais que podem variar o tratamento das crianças em diferentes estados. Ao realizar um estudo de textos relacionados aos direitos das crianças, é importante identificar os termos, seus conceitos e termos/conceitos relacionados. Vocabulários controlados e normas técnicas podem ajudar a lidar com a variação terminológica. Isso é semelhante à padronização de terminologias na área da Saúde, como a Classificação Internacional de Doenças (CID).\", 'sentence_results': [{'sentence': \"A palavra 'comum' assume diferentes significados no contexto do Estatuto da Criança e do Adolescente (ECA), que define criança como pessoa até 12 anos incompletos e adolescente como pessoa entre 12 e 18 anos.\", 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['comum', 'Estatuto da Criança e do Adolescente', 'ECA', 'criança', 'adolescente']}}, {'sentence': 'Isso mostra a importância de considerar o contexto jurídico ao estudar o tema das crianças em leis e documentos em português.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['importância', 'contexto jurídico', 'estudar', 'tema', 'crianças', 'leis', 'documentos', 'português']}}, {'sentence': 'Além disso, o ECA está subordinado à Constituição do Brasil e existem leis locais que podem variar o tratamento das crianças em diferentes estados.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['ECA', 'Constituição', 'leis locais', 'tratamento das crianças', 'estados']}}, {'sentence': 'Ao realizar um estudo de textos relacionados aos direitos das crianças, é importante identificar os termos, seus conceitos e termos/conceitos relacionados.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['direitos das crianças', 'infância', 'proteção', 'educação', 'saúde', 'alimentação', 'abuso', 'exploração', 'violência', 'negligência']}}, {'sentence': 'Vocabulários controlados e normas técnicas podem ajudar a lidar com a variação terminológica.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['vocabulários controlados', 'normas técnicas', 'variação terminológica']}}, {'sentence': 'Isso é semelhante à padronização de terminologias na área da Saúde, como a Classificação Internacional de Doenças (CID).', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['padronização', 'terminologias', 'Saúde', 'Classificação Internacional de Doenças', 'CID']}}], 'final_sentiment_count': {'positive': 1, 'negative': 0, 'neutral': 5, 'mixed': 0}, 'SSCORE': 16.666666666666664}, {'text': 'para o termo SARAMPO e seus tipos – uma doença , no Brasil , geralmente associada a crianças . Dada a relevância e necessidade de tratar esse assunto , alguns tribunais como o Supremo Tribunal Federal e Superior Tribunal de Justiça criaram um site denominado “ Tesauro ” como forma de ferramenta para controle terminológico que tem por objetivo a padronização da informação . Nesta ferramenta , o tesauro , são apresentados os termos , conceitos , termos relacionados , mas também categorias , termos genéricos e termos específicos . A partir deste mapeamento , é possível orientar que os servidores públicos redijam os documentos judiciais com uma terminologia uniforme , para auxiliar na pesquisa e recuperação da informação posteriormente . Para saber mais sobre o tema dos tesauros e sua interface com as terminologias , vale consultar o trabalho de Vargas ; Van der Lann ( 2011 ) . Dado o contexto que os tribunais pertencem , há uma variedade maior de termos relacionados em comparação à legislação e cada tribunal pode apresentar informações diversas nos tesauros para o mesmo termo . Tesauros são listas de assuntos , palavras-chave e de terminologias de uma dada área de conhecimento . Essas listagens dão suporte à indexação e catalogação de documentos em bibliotecas e em diferentes acervos , como bases de dados . Geralmente , quem produz esses tesauros são os bibliotecários , documentalistas e cientistas da informação que lidam com a catalogação de informações técnicas e científicas . Veja este exemplo , quando se busca pelo item CRIANÇA no tesauro do Supremo Tribunal Federal . Nessa busca , temos o seguinte resultado : O que se informa aqui não é um conceito para CRIANÇA , mas se aponta que ele tem um correspondente ou equivalente genérico nesse âmbito . Isto é , CRIANÇA = MENOR ( genérico ) . Em seguida , nos termos relacionados , vemos assuntos em que se inclui esse item . Feitas essas explicações sobre peculiaridades das terminologias , em suas diferentes circunstâncias e variabilidades de uso e de significações , um RT pode ser visto como um tipo de trabalho de mediação de comunicação , realizado por profissionais de uma área , terminólogos , linguistas , informatas , entre outros . Salienta-se , assim , a ideia de uma mediação terminológica ( Conceição ; Zanola , 2020 ) . O RT pode ser um trabalho multidocumento e multitemático . Pode , ainda , apontar ligações entre documentos de diferentes naturezas , extrapolando-se o reconhecimento de um dado tópico para diferentes fronteiras . Um exemplo seriam os materiais sobre temas e políticas de Saúde Pública voltadas para crianças e os documentos jurídicos que estabelecem seus direitos . Outro exemplo de trabalho seria verificar como determinado tribunal interpreta e aplica a legislação sobre crianças , diante de problemas específicos . Um RT legislativo também poderia servir de apoio para um recurso didático voltado para o cidadão comum , sem formação em Direito , ou mesmo para diferentes estudantes universitários interessados na legislação ambiental do Brasil . Nesse caso , vamos imaginar um conjunto composto , por exemplo , por 800 leis , as quais versam sobre diferentes aspectos ambientais . Vamos supor que estamos trabalhando em um RT para uso de jornalistas que lidam com temas ambientais . Como explorar essas 800 leis para chegar , por exemplo , a um conjunto de seus termos e conceitos conforme sejam mais comumente empregados nessas leis ? Como apresentar a informação de forma a melhor atender o nosso suposto usuário jornalista , que , sem ter formação em Direito ou Biologia , precisaria ler e entender a legislação ? Bastaria perguntar ao ChatGPT ? Naturalmente , hoje , dada a larga prática de digitalização desse tipo de documento e a garantia de seu acesso a todo o cidadão , parece ser fácil encontrar e percorrer uma base de dados com leis ambientais . O Senado Federal do Brasil , por exemplo , oferece todo um banco de leis , decretos e outros documentos afins para acesso público . Basta a pessoa acessar um site determinado e salvar os documentos no seu computador . Feito isso , “ bastaria ” a pessoa – o jornalista que imaginamos – ler , calmamente e com cuidado , todas as 800 leis do nosso caso imaginário e ir fazendo um registro , em um arquivo de texto , de suas terminologias e conceituações à medida que avance com a leitura . Outra opção seria o “ nosso ” jornalista consultar um dicionário especializado sobre esse tema , mas é nele , como um ponto final possível , entre outros , de um RT que queremos chegar com o que tratamos neste texto . Como a legislação em alguns aspectos pode ser principiológica ou apenas fornecer diretrizes , ou ainda possuir conflitos de termos entre normas diversas , pode ser necessário associar um outro RT para identificar o entendimento prevalente . Neste caso , pode ser associado a um RT Judicial , como é o caso dos tesauros dos tribunais , mencionados anteriormente . Naturalmente , além dessas fontes padronizadas , há dicionários e glossários descritivos sobre o tema do Direito Ambiental do Brasil . Um exemplo é o Dicionário de Direito Ambiental do Grupo Termisul da UFRGS , publicado em segunda edição em 2008 . A produção desse dicionário demandou construir , desde 1994 , toda uma base legislativa sobre temas do meio ambiente associada à obra , a Base Legis2 . Essa base , que começa com o texto do Código de Águas do Brasil , de 19303 . Mas , voltando aos tesauros , segundo o tesauro do Supremo Tribunal Federal , temos as seguintes informações : Como termo , há poucas informações sobre Direito Ambiental , mas ao acessar a categoria “ DAM Direito Ambiental ” , temos cerca de 200 termos relacionados . Vale destacar que os termos apresentados fazem parte do contexto ao qual o tribunal pertence e', 'summarized_text': \"O Sarampo é uma doença no Brasil geralmente associada a crianças. Os tribunais como o Supremo Tribunal Federal e Superior Tribunal de Justiça criaram um site denominado 'Tesauro' para padronizar a informação. Tesauros são listas de assuntos e terminologias de uma área de conhecimento. Essas listagens são usadas na indexação e catalogação de documentos. O trabalho de um Terminólogo (RT) envolve a mediação terminológica e pode incluir a análise de documentos jurídicos e legislação. Os tesauros dos tribunais oferecem informações sobre termos relacionados ao Direito Ambiental. Há também dicionários especializados nessa área, como o Dicionário de Direito Ambiental do Grupo Termisul da UFRGS.\", 'sentence_results': [{'sentence': 'O Sarampo é uma doença no Brasil geralmente associada a crianças.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['Sarampo', 'doença', 'Brasil', 'crianças']}}, {'sentence': \"Os tribunais como o Supremo Tribunal Federal e Superior Tribunal de Justiça criaram um site denominado 'Tesauro' para padronizar a informação.\", 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['tribunais', 'Supremo Tribunal Federal', 'Superior Tribunal de Justiça', 'site', 'Tesauro', 'padronizar', 'informação']}}, {'sentence': 'Tesauros são listas de assuntos e terminologias de uma área de conhecimento.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['sinônimos', 'antônimos', 'hiperônimos', 'hipônimos', 'relacionados', 'traduções', 'definições']}}, {'sentence': 'Essas listagens são usadas na indexação e catalogação de documentos.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['indexação', 'catalogação', 'documentos']}}, {'sentence': 'O trabalho de um Terminólogo (RT) envolve a mediação terminológica e pode incluir a análise de documentos jurídicos e legislação.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['Terminólogo', 'mediação terminológica', 'análise de documentos jurídicos', 'legislação']}}, {'sentence': 'Os tesauros dos tribunais oferecem informações sobre termos relacionados ao Direito Ambiental.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['tesauro', 'tribunais', 'Direito Ambiental']}}, {'sentence': 'Há também dicionários especializados nessa área, como o Dicionário de Direito Ambiental do Grupo Termisul da UFRGS.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['Dicionário de Direito Ambiental', 'Grupo Termisul', 'UFRGS']}}], 'final_sentiment_count': {'positive': 4, 'negative': 0, 'neutral': 3, 'mixed': 0}, 'SSCORE': 57.14285714285714}, {'text': \"os processos judiciais que julga . Um RT legislativo pode ter mais termos que os apresentados no tesauro do Supremo Tribunal Federal , mas em contrapartida , o tesauro do tribunal , pode ter um nível de detalhamento maior . Já para o Superior Tribunal de Justiça do Brasil , ao se consultar o termo/assunto Direito Ambiental , temos os seguintes resultados : A vantagem do tesauro elaborado por alguns tribunais é que esse instrumento reduz esforços na aplicação do PLN ao Direito , uma vez que já associa os termos mais frequentes nas decisões judiciais e associa esses termos às decisões judiciais existentes . Dado que muitos dos tesauros não associam conceitos aos termos , ao trabalhar com RT de outras fontes , como o legislativo , podem ser obtidas estas informações . A análise de sentimento em textos jurídicos envolve a aplicação de técnicas de PLN para determinar o tom emocional ou opinativo presente nos documentos legais . Dentre diferentes possibilidades de aplicação da técnica de análise de sentimento em textos jurídicos , podemos trabalhar com sentenças judiciais . Assim , analisa-se o contexto das sentenças judiciais e identifica-se se o juiz foi favorável ou desfavorável ao pedido de cada parte . Para esse tipo de trabalho , alguns passos devem ser seguidos como : 1 ) Coleta de Dados ; 2 ) Pré-Processamento do Texto ; 3 ) Rotulação de Dados ; 4 ) Escolha da técnica de análise de sentimento ; 5 ) Execução da técnica e 6 ) Avaliação e Validação . Na coleta de dados , é necessário escolher um repositório que possua os textos das decisões judiciais , sejam elas de forma resumida ou na íntegra . Dentre as opções públicas e gratuitas , estão os diários de justiça dos tribunais , uso de APIs ( Application Programming Interface ) públicas como o DataJud do Conselho Nacional de Justiça e decisões disponibilizadas nos sistemas de busca dos tribunais . Para automatização desta coleta , é preciso o conhecimento de técnicas de web crawling e web scraping ( Macohin ; Carneiro , 2020 ) . Essas técnicas consistem na automatização do download das páginas e arquivos que possuem decisões judiciais e posterior filtragem da informação que se deseja usar ) , respectivamente . Um exemplo de informação que pode ser obtida de um tribunal pode ser verificado abaixo . Neste exemplo foram suprimidas algumas informações que pudessem identificar os envolvidos . Veja que lidamos com um tipo de texto que contém uma parte denominada ementa e outra parte que é o acórdão , com o resultado do processo . A partir do download desta página , o objetivo é extrair a informação do acórdão , último parágrafo da imagem , onde consta se foi dado ou negado provimento ao pedido do autor . Neste caso , foi negado provimento ao autor do recurso , como se verifica através do uso das palavras “ negou provimento ” . Em posse dos trechos das decisões judiciais que se deseja analisar , de forma automatizada , se o desfecho foi favorável ou desfavorável , pode-se iniciar a fase de pré-processamento do texto . A fase de pré-processamento pode contemplar diversas subtécnicas . Mas , para fins de exemplificação , vamos citar apenas a tokenização , a remoção de pontuações , conversão de todas as letras para minúsculas e remoção de stop words . A tokenização consiste em dividir o texto em palavras ou unidades menores , chamadas de tokens ( veja mais detalhes no Capítulo 4 ) , que são conjuntos de caracteres separados por um espaço em branco . Um critério para separação dos tokens pode ser o espaço entre as palavras . Já a remoção de pontuações visa eliminar pontuação e caracteres especiais que não são relevantes para a análise de sentimento . Em seguida , a conversão para minúsculas consiste em transformar todas as palavras em minúsculas para garantir consistência nas comparações . Por fim , a remoção de stop words consiste na remoção de palavras que são comuns - as palavras gramaticais ou instrumentais - e não contribuem significativamente para uma análise de sentimento , como “ a ” , “ o ” , “ em ” , “ por ” etc . Vale destacar que a lista de stop words deve ser na mesma língua do texto analisado . Veja o Quadro 22.1 abaixo . Quadro 22.1 Exemplo de pré-processamento de um trecho de decisão judicial . Texto Original : “ Vistos , relatados e discutidos esses autos em que são partes as acima indicadas , acordam os Ministros da SEGUNDA TURMA do Superior Tribunal de Justiça , na conformidade dos votos e das notas taquigráficas , o seguinte resultado de julgamento : 'Prosseguindo-se no julgamento , após o voto-vista do Sr. Ministro Francisco Falcão , acompanhando o Sr. Ministro-Relator , dando provimento ao recurso especial , a Turma , por maioria , negou provimento ao recurso especial , nos termos do voto do Sr. Ministro Og Fernandes , vencidos os Srs . Ministros Herman Benjamin e Francisco Falcão . Lavrará o acórdão o Sr. Ministro Mauro Campbell Marques . ' Os Srs . Ministros Mauro Campbell Marques e Assusete Magalhães votaram com o Sr. Ministro Og Fernandes. ” Após Pré-processamento : “ vistos relatados discutidos autos partes acima indicadas acordam ministros segunda turma superior tribunal justiça conformidade votos notas taquigráficas seguinte resultado julgamento prosseguindo julgamento após voto-vista sr ministro francisco falcão acompanhando sr ministro-relator dando provimento recurso especial turma maioria negou provimento recurso especial termos voto sr ministro og fernandes vencidos srs ministros herman benjamin francisco falcão lavrará acórdão sr ministro mauro campbell marques srs ministros mauro campbell marques assusete magalhães votaram sr ministro og fernandes ” Ainda na fase de pré-processamento , é possível aperfeiçoar a tarefa e incluir novas stop words , com o objetivo de limpar mais ainda o texto e facilitar futuramente a identificação das palavras positivas ou negativas na sentença . No Quadro 22.1 , verifica-se\", 'summarized_text': 'Os processos judiciais podem ser julgados com base em termos do legislativo, mas o tesauro do Supremo Tribunal Federal tem um nível de detalhamento maior. O tesauro elaborado por alguns tribunais reduz esforços na aplicação do Processamento de Linguagem Natural (PLN) ao Direito, pois já associa os termos mais frequentes nas decisões judiciais. A análise de sentimento em textos jurídicos envolve a determinação do tom emocional ou opinativo presente nos documentos legais. Para isso, são seguidos passos como coleta de dados, pré-processamento do texto, rotulação de dados, escolha da técnica de análise de sentimento, execução da técnica e avaliação e validação. A coleta de dados pode ser feita em diários de justiça, APIs públicas como o DataJud do Conselho Nacional de Justiça e sistemas de busca dos tribunais. O pré-processamento do texto envolve a tokenização, remoção de pontuações, conversão para minúsculas e remoção de stop words. Após o pré-processamento, pode-se aperfeiçoar a tarefa incluindo novas stop words para limpar ainda mais o texto.', 'sentence_results': [{'sentence': 'Os processos judiciais podem ser julgados com base em termos do legislativo, mas o tesauro do Supremo Tribunal Federal tem um nível de detalhamento maior.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['processos judiciais', 'legislativo', 'tesauro', 'Supremo Tribunal Federal']}}, {'sentence': 'O tesauro elaborado por alguns tribunais reduz esforços na aplicação do Processamento de Linguagem Natural (PLN) ao Direito, pois já associa os termos mais frequentes nas decisões judiciais.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['tesauro', 'tribunais', 'PLN', 'Direito', 'processamento de linguagem natural', 'decisões judiciais']}}, {'sentence': 'A análise de sentimento em textos jurídicos envolve a determinação do tom emocional ou opinativo presente nos documentos legais.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['análise de sentimento', 'textos jurídicos', 'tom emocional', 'opinativo', 'documentos legais']}}, {'sentence': 'Para isso, são seguidos passos como coleta de dados, pré-processamento do texto, rotulação de dados, escolha da técnica de análise de sentimento, execução da técnica e avaliação e validação.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['coleta de dados', 'pré-processamento', 'rotulação de dados', 'análise de sentimento', 'execução da técnica', 'avaliação e validação']}}, {'sentence': 'A coleta de dados pode ser feita em diários de justiça, APIs públicas como o DataJud do Conselho Nacional de Justiça e sistemas de busca dos tribunais.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['diários de justiça', 'APIs públicas', 'DataJud', 'Conselho Nacional de Justiça', 'sistemas de busca dos tribunais']}}, {'sentence': 'O pré-processamento do texto envolve a tokenização, remoção de pontuações, conversão para minúsculas e remoção de stop words.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['pré-processamento', 'texto', 'tokenização', 'remoção de pontuações', 'conversão para minúsculas', 'remoção de stop words']}}, {'sentence': 'Após o pré-processamento, pode-se aperfeiçoar a tarefa incluindo novas stop words para limpar ainda mais o texto.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['pré-processamento', 'stop words', 'limpar', 'texto']}}], 'final_sentiment_count': {'positive': 4, 'negative': 0, 'neutral': 3, 'mixed': 0}, 'SSCORE': 57.14285714285714}, {'text': 'que as palavras “ srs ” , “ sr ” , “ vistos ” , não influenciam na interpretação da decisão judicial e podem ser removidas . A próxima fase , rotulamento de dados , consiste em classificar manualmente algumas decisões como positivas ou negativas , para fins de validação futura se a classificação automatizada está desempenhando um bom resultado . A partir do Quadro 22.1 , facilmente esta decisão seria classificada como “ NEGATIVA ” . Outras opções de rótulo seriam “ POSITIVA ” e “ NEUTRA ” . Os casos de neutro poderiam ser utilizados , por exemplo , quando o juiz decidiu parcialmente pelo provimento . Já a fase da escolha da técnica de análise de sentimento , consiste em selecionar qual abordagem será utilizada , se baseada em regras ou baseada em aprendizado de máquina . Na abordagem baseada em regras , é criado um conjunto de regras e heurísticas que determinam o sentimento ou polaridade com base em palavras-chave , padrões gramaticais e outras características linguísticas . Por exemplo , certas palavras negativas podem indicar um sentimento negativo . Já , na abordagem baseada em aprendizado de máquina , é treinado um modelo de aprendizado de máquina usando-se os dados linguísticos rotulados . Algoritmos como Naïve Bayes , Support Vector Machines ( SVM ) ou redes neurais podem ser usados para construir um modelo . Para dar continuidade ao exemplo mencionado , utilizaremos a abordagem baseada em regras . Nesse caso , utilizamos um dicionário prévio com palavras positivas e negativas . Quando são usados dicionários , deve ser considerada a língua do texto . Como exemplo de dicionário de palavras positivas , negativas e neutras em português , temos o SentiLex-PT4 ( Carvalho ; Silva , 2017 ) . Na fase de execução da técnica e a partir do texto pré-processado anteriormente , cada palavra do texto é verificada se consta no dicionário como palavra positiva , negativa ou neutra . Segundo o SentiLex-PT , foi encontrado o seguinte resultado apresentado no Quadro 22.2 . Quadro 22.2 Palavras positivas e negativas segundo o SentiLex-PT . Dicionário : Positivas : [ “ acordam ” , “ conformidade ” , “ aprovar ” ] Negativas : [ “ negou ” , “ vencidos ” ] Sentimento : Positivas : 3 ( acordam , conformidade , aprovar ) Negativas : 2 ( negou , vencidos ) Sentimento Geral : 1 ( POSITIVO ) ( Positivas - Negativas = 3 - 2 ) O sentimento geral é calculado realizando uma subtração do número de palavras positivas com o número de palavras negativas . Um resultado com valor positivo indica um sentimento positivo , já um resultado com valor negativo indica um sentimento negativo e um valor próximo de zero indica um sentimento neutro . Este cálculo pode ser aperfeiçoado ao dividir o número encontrado pelo total de palavras ( tokens ) existentes no texto ( ( palavras positivas - palavras negativas ) / total de palavras ) . Ou seja , se há 10 palavras no texto , 3 são positivas e 0 negativas , indica uma maior “ probabilidade ” que o texto realmente seja positivo . Por outro lado , se há 50 palavras no texto , apenas 1 negativa e nenhuma positiva , há probabilidade de ser um falso negativo . Essa divisão pode indicar que novos aperfeiçoamentos no dicionário podem ser necessários . Neste exemplo , verifica-se que o resultado não reflete a resposta correta ( NEGATIVO ) e ajustes devem ser feitos . O dicionário SentiLex-PT pode ser adaptado para a linguagem jurídica , uma vez que “ acordam ” “ conformidade ” e “ aprovar ” , não indicam necessariamente que o juiz está dando provimento ( julgando como positivo ) a decisão judicial , logo , devem ser desassociados do sentimento “ POSITIVO ” . Outro ajuste que pode ser feito também é não associar a palavra “ vencidos ” ao sentimento “ NEGATIVO ” , uma vez que é comum , quando há divergência entre o grupo de juízes votantes , aparecer a palavra “ vencidos ” . Outro ajuste que também pode ser feito no dicionário é associar palavras frequentemente encontradas juntas e que reflitam a intenção da decisão judicial , por exemplo “ negou provimento ” , “ negado provimento ” , “ não provido o recurso ” , ” não dado provimento ” , entre outros . Feitos estes ajustes , teremos o seguinte resultado apresentado no Quadro 22.3 . Quadro 22.3 Resultado após os ajustes . Dicionário : Positivas : [ “ ” ] Negativas : [ “ negou provimento ” ] Sentimento : Positivas : 0 ( ) Negativas : 1 ( negou provimento ) Sentimento Geral : -1 ( NEGATIVO ) ( Positivas - Negativas = 0 - 1 ) Lembrando de que essa abordagem é uma simplificação e pode não capturar todas as nuances de sentimento e/ou as polaridades em textos jurídicos complexos . Principalmente quando há uma variedade de pedidos sendo julgados com decisões diferentes para cada pedido . O contexto legal específico também pode influenciar a interpretação das palavras . Portanto , ajustes e validações são sempre necessários . Por fim , com relação à fase de avaliação e validação , se foi utilizada a abordagem baseada em regras , como a acima exemplificada , é mais simples validar com a amostra rotulada previamente e comparar a taxa de acertos e erros . Já no caso da abordagem baseada em aprendizado de máquina , é possível utilizar parâmetros estatísticos para demonstrar a precisão e desempenho do modelo . Como mencionado anteriormente , é necessário fazer ajustes em cada fase da execução da análise de sentimento devido às peculiaridades discursivas dos textos jurídicos que nem sempre constam nos dicionários existentes . A partir das informações obtidas na última fase , de avaliação e validação , pode sugerir que novos aperfeiçoamentos sejam feitos nas fases anteriores , para que o algoritmo tenha um', 'summarized_text': 'Neste texto, é discutido o processo de análise de sentimento em decisões judiciais. São apresentadas três fases do processo: remoção de palavras irrelevantes, rotulamento de dados e escolha da técnica de análise de sentimento. Na fase de escolha da técnica, são mencionadas duas abordagens: baseada em regras e baseada em aprendizado de máquina. No exemplo dado, é utilizada a abordagem baseada em regras, com o uso de um dicionário prévio de palavras positivas e negativas. Após a execução da técnica, é calculado um sentimento geral, que pode ser positivo, negativo ou neutro. O texto ressalta que o processo de análise de sentimento em textos jurídicos é complexo devido às nuances e peculiaridades discursivas, e são necessários ajustes e validações em cada fase do processo.', 'sentence_results': [{'sentence': 'Neste texto, é discutido o processo de análise de sentimento em decisões judiciais.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['processo', 'análise de sentimento', 'decisões judiciais']}}, {'sentence': 'São apresentadas três fases do processo: remoção de palavras irrelevantes, rotulamento de dados e escolha da técnica de análise de sentimento.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['remoção de palavras irrelevantes', 'rotulamento de dados', 'escolha da técnica de análise de sentimento']}}, {'sentence': 'Na fase de escolha da técnica, são mencionadas duas abordagens: baseada em regras e baseada em aprendizado de máquina.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['baseada em regras', 'baseada em aprendizado de máquina']}}, {'sentence': 'No exemplo dado, é utilizada a abordagem baseada em regras, com o uso de um dicionário prévio de palavras positivas e negativas.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['abordagem baseada em regras', 'dicionário prévio', 'palavras positivas', 'palavras negativas']}}, {'sentence': 'Após a execução da técnica, é calculado um sentimento geral, que pode ser positivo, negativo ou neutro.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['positivo', 'negativo', 'neutro']}}, {'sentence': 'O texto ressalta que o processo de análise de sentimento em textos jurídicos é complexo devido às nuances e peculiaridades discursivas, e são necessários ajustes e validações em cada fase do processo.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['análise de sentimento', 'textos jurídicos', 'complexo', 'nuances', 'peculiaridades discursivas', 'ajustes', 'validações', 'fase do processo']}}], 'final_sentiment_count': {'positive': 0, 'negative': 0, 'neutral': 6, 'mixed': 0}, 'SSCORE': 0.0}, {'text': 'desempenho similar e até superior ao de uma atividade humana . Dentre os desafios para aplicar este tipo de técnica em decisões judiciais , está principalmente na coleta dos dados . Os tribunais , no geral , não possuem repositórios com estas informações prontas , estruturadas e públicas e isto por si só , já dificulta iniciar qualquer trabalho de processamento de linguagem natural . Apesar do problema poder ser contornado com a criação de algoritmos de web crawling e web scraping , alguns tribunais fazem uso de captchas que impedem o acesso automatizado e massivo às informações . Apesar de iniciativas do Conselho Nacional de Justiça , como o DataJud , para centralizar e fornecer informações estruturadas por meio de uma API , ainda não há a íntegra das decisões disponibilizadas . Entretanto , como o DataJud continua em constante evolução , é possível que futuramente seja disponibilizado . A limpeza e seleção das informações contidas em uma página HTML ou arquivo PDF também é bastante custosa e somente a partir destes esforços torna possível dar seguimento à aplicação da técnica de análise de sentimento em Direito . Este capítulo tentou situar o “ mundo textual ” oferecido pelo Direito do Brasil e trazer alguns exemplos de trabalhos e estudos com a sua linguagem e as suas práticas de escrita . Comentamos um pouco dos estudos históricos com processos brasileiros antigos sobre crimes contra mulheres , mencionamos que a tarefa de RT é uma demanda no Direito Ambiental e trouxemos um breve exemplo de estudo de sentimentos ou de polaridades , em PLN , para ajudar a detectar os tipos de decisões que os juízes brasileiros tomam em determinados tipos de processos federais . Buscamos salientar que é importante considerar as características e elementos de diferentes tipos de documentos , sejam leis , códigos , processos ou sentenças , produzidos por diferentes instâncias jurídicas . E , visto esse panorama , caso você possa se interessar especificamente por sentenças de tribunais administrativamente menores , como os Juizados Especiais Cíveis ( JECs ) - conhecidos como “ tribunais de pequenas causas ” , vale conhecer o trabalho de doutorado de Motta ( 2022 ) . A autora estudou a complexidade das sentenças dos JECs , quanto ao vocabulário , terminologias e sintaxe , frente aos preceitos da legislação que estabelece que tais sentenças devem ser escritas em linguagem simples , que possibilitem fácil compreensão sobre o que se decide em uma causa . Afinal , o cidadão comum recorre aos JECs geralmente sem advogados , em meio a causas de valor limitado . Além de ampla análise , Motta ( 2022 ) oferece , no seu trabalho , acesso a todo um corpus de sentenças por ela reunido e analisado com a ferramenta NILC-METRIX5 ( Leal et al. , 2021 ) . Também os corpora que ela usou como contraponto para ponderar a complexidade/facilidade de linguagem dessas sentenças estão disponíveis nos seus anexos . O Direito é um mundo feito de palavras e modos de dizer , o que oferece um terreno fértil para os nossos trabalhos de análise linguístico-textual , em geral , e , em especial , para diferentes tarefas do PLN . Os resultados desses trabalhos beneficiam tanto os profissionais quanto o cidadão e a sociedade , que são os principais focos e beneficiários das ações do Direito . Veja mais sobre a delimitação de “ palavras ” e de unidades de processamento no Capítulo 4 . Neste capítulo sobre Direito e PLN , vamos acentuar a noção de palavra como uma unidade da língua escrita , situada entre dois espaços em branco , ou entre espaço em branco e sinal de pontuação.↩︎ A Base Legis Termisul-UFRGS é composta de textos da Legislação Ambiental do Brasil , Alemanha , Argentina , Estados Unidos , França , Paraguai e Uruguai . Também inclui códigos brasileiros , constituições dos países anteriormente mencionados e dos demais países de fala portuguesa ( Angola , Cabo Verde , Guiné Bissau , Moçambique , Portugal , São Tomé e Príncipe e Timor Leste ) , Atos Internacionais relativos ao meio ambiente ( Agenda 21 , Convenção de Estocolmo , Declaração do Rio e Protocolo de Kyoto ) . Todos os textos possuem uma descrição e podem ser baixados em formato TXT.↩︎ Você pode acessar em http : //www.ufrgs.br/termisul na aba “ Recursos ” .↩︎ O SentiLex-PT está disponível em : https : //b2share.eudat.eu/records/93ab120efdaa4662baec6adee8e7585f.↩︎ http : //fw.nilc.icmc.usp.br:23380/nilcmetrix↩︎', 'summarized_text': 'O uso de processamento de linguagem natural no campo do Direito enfrenta desafios na coleta de dados, já que os tribunais não possuem repositórios com informações estruturadas e públicas. Algoritmos de web crawling e web scraping podem contornar esse problema, mas alguns tribunais utilizam captchas para impedir acesso automatizado. A limpeza e seleção das informações também são custosas. Mesmo assim, existem iniciativas como o DataJud, do Conselho Nacional de Justiça, que centraliza e fornece informações estruturadas para análise. O capítulo destaca a importância de considerar as características e elementos de diferentes tipos de documentos jurídicos. Também menciona um estudo sobre a complexidade das sentenças dos Juizados Especiais Cíveis e oferece acesso a um corpus de sentenças analisadas. O capítulo conclui ressaltando que o Direito é um campo fértil para o trabalho de análise linguístico-textual, beneficiando profissionais e a sociedade como um todo.', 'sentence_results': [{'sentence': 'O uso de processamento de linguagem natural no campo do Direito enfrenta desafios na coleta de dados, já que os tribunais não possuem repositórios com informações estruturadas e públicas.', 'print_sentiment': {'sentiment': 'negative'}, 'print_keywords': {'keywords': ['processamento de linguagem natural', 'Direito', 'desafios', 'coleta de dados', 'tribunais', 'informações estruturadas', 'públicas']}}, {'sentence': 'Algoritmos de web crawling e web scraping podem contornar esse problema, mas alguns tribunais utilizam captchas para impedir acesso automatizado.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['web crawling', 'web scraping', 'captchas', 'tribunais']}}, {'sentence': 'A limpeza e seleção das informações também são custosas.', 'print_sentiment': {'sentiment': 'negative'}, 'print_keywords': {'keywords': ['limpeza', 'seleção', 'informações', 'custosas']}}, {'sentence': 'Mesmo assim, existem iniciativas como o DataJud, do Conselho Nacional de Justiça, que centraliza e fornece informações estruturadas para análise.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['DataJud', 'Conselho Nacional de Justiça', 'informações estruturadas', 'análise']}}, {'sentence': 'O capítulo destaca a importância de considerar as características e elementos de diferentes tipos de documentos jurídicos.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['importância', 'considerar', 'características', 'elementos', 'documentos jurídicos']}}, {'sentence': 'Também menciona um estudo sobre a complexidade das sentenças dos Juizados Especiais Cíveis e oferece acesso a um corpus de sentenças analisadas.', 'print_sentiment': {'sentiment': 'neutral'}, 'print_keywords': {'keywords': ['estudo', 'complexidade', 'sentenças', 'Juizados Especiais Cíveis', 'corpus']}}, {'sentence': 'O capítulo conclui ressaltando que o Direito é um campo fértil para o trabalho de análise linguístico-textual, beneficiando profissionais e a sociedade como um todo.', 'print_sentiment': {'sentiment': 'positive'}, 'print_keywords': {'keywords': ['Direito', 'análise linguística-textual', 'profissionais', 'sociedade']}}], 'final_sentiment_count': {'positive': 2, 'negative': 2, 'neutral': 3, 'mixed': 0}, 'SSCORE': 0.0}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Capítulo 4 - Sequência de caracteres e palavras"
      ],
      "metadata": {
        "id": "n3tUBxANpa0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Número de tokens do texto original**"
      ],
      "metadata": {
        "id": "ocxkPYIlpkQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_chapter_4 = data[\"Capítulo 4: Sequência de caracteres e palavras\"]\n",
        "no_tokens_original_text = 0\n",
        "\n",
        "for text_chunk in result_chapter_4:\n",
        "  tokens = nltk.word_tokenize(text_chunk[\"text\"])\n",
        "  no_tokens_original_text += len(tokens)\n",
        "\n",
        "print(no_tokens_original_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9AfR80fpXXZ",
        "outputId": "a3305160-8fcd-441a-f0d9-c0065ff99388"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Número de tokens do texto resumido**"
      ],
      "metadata": {
        "id": "iM_w_QdgqZfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "no_tokens_summarized_text = 0\n",
        "\n",
        "for text_chunk in result_chapter_4:\n",
        "  tokens = nltk.word_tokenize(text_chunk[\"summarized_text\"])\n",
        "  no_tokens_summarized_text += len(tokens)\n",
        "\n",
        "print(no_tokens_summarized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKF6ftA1qNEd",
        "outputId": "1548ba2f-ecc8-48e8-fd32-7acf3a14af07"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Texto resumido**"
      ],
      "metadata": {
        "id": "_-J_1qcOqj1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarized_text = \"\"\n",
        "\n",
        "for text_chunk in result_chapter_4:\n",
        "  summarized_chunk = text_chunk[\"summarized_text\"]\n",
        "  summarized_text += summarized_chunk + \"\\n\"\n",
        "\n",
        "print(summarized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DK2JGf6BqiBj",
        "outputId": "fb5b4e2d-33cf-48c7-d953-9f3dea46e161"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neste capítulo, são abordadas questões relacionadas à identificação da unidade mínima na linguagem, tanto no campo computacional quanto na linguística. A delimitação dessa unidade não é consensual, variando de acordo com o foco e ponto de vista dos pesquisadores. A fonologia considera o fonema como a menor unidade sonora, enquanto a morfologia considera o morfema como a menor unidade dotada de significado. Além disso, há controvérsias sobre o que é considerado uma palavra, seja pela sua composição sonora, escrita ou mesmo pela combinação de palavras. Também são discutidas as questões relacionadas a abreviaturas, siglas, interjeições e outros elementos linguísticos presentes nas redes sociais. No campo do processamento de linguagem natural, a definição da unidade de processamento depende das necessidades da tarefa ou trabalho em questão. Por fim, também é abordado o conceito de palavra computacional, adaptada ou criada para facilitar o processamento por máquinas, levando em consideração a complexidade e ambiguidade da linguagem humana.\n",
            "Neste texto, é discutido o uso de acentos ortográficos, espaços em branco e hífens em palavras. Além disso, são apresentadas estratégias para lidar com expressões multipalavras, hashtags, URLs e outros compostos como palavras únicas. Também são definidos conceitos linguísticos básicos relacionados ao processamento de texto. O texto aborda ainda as tarefas relacionadas ao processamento morfológico e indica as ferramentas e recursos disponíveis para o português. Por fim, é feita uma retomada dos principais tópicos discutidos e são apresentadas considerações finais e planos para futuras versões. São explicados os conceitos de morfema, token, type, lexema, lexia, lema, léxico e gramática. São fornecidas definições e exemplos de diferentes tipos de morfemas em português, como desinências, raiz, radical, afixo, vogal temática e tema.\n",
            "O radical de uma palavra é o morfema nuclear que expressa sua base significativa. Os afixos são morfemas lexicais que se juntam ao radical para formar uma nova palavra. A vogal temática aparece imediatamente após o radical. O tema é a combinação do radical e da vogal temática. Os morfemas podem ser classificados como lexicais (raiz, radical, tema) ou gramaticais (desinências, afixos, vogais temáticas). A tokenização divide um texto em unidades significativas, chamadas tokens. O type refere-se aos tokens únicos em um texto. A proporção token/type indica a riqueza lexical do texto. O lexema é a forma de entrada de um verbete no dicionário.\n",
            "Um lexema é uma forma abstrata de uma palavra, enquanto uma lexia é a forma concreta que um lexema assume no discurso. O lema é a representação das propriedades sintático-semânticas de um item lexical. O léxico corresponde ao conjunto de palavras de uma língua, juntamente com suas definições morfossintáticas. O léxico comum contém as palavras mais comumente usadas em uma língua, mas sempre é possível ter palavras ausentes do léxico.\n",
            "O léxico de uma língua consiste nas palavras comuns que não possuem um conceito técnico-científico específico, enquanto o léxico especializado contém termos que têm significados específicos em áreas de conhecimento ou ciência. Alguns termos técnicos podem se tornar palavras comuns e vice-versa. As palavras funcionais/gramaticais são um grupo fechado, enquanto as palavras lexicais são um grupo aberto que permite a criação de novas palavras. A formação de novas palavras pode ocorrer por derivação ou composição.\n",
            "A morfologia é o ramo da linguística que estuda os morfemas e como eles se combinam para formar as palavras. A morfossintaxe, por sua vez, explora como as escolhas morfológicas afetam a estrutura das frases. Ambas as áreas são importantes no processamento de linguagem natural. No pré-processamento de texto, tarefas como segmentação de sentenças, tokenização e etiquetagem morfossintática são realizadas. A sentenciação é a segmentação do texto em sentenças, que pode ser feita usando pontuações delimitadoras. No entanto, a ambiguidade das línguas e o uso de pontuações em abreviações e numerais tornam o processo desafiador.\n",
            "O problema de segmentação automática de textos, ainda que explorado desde o início pela área de PLN, é bastante desafiador e ainda está em aberto. Atualmente utilizam-se três tipos de abordagens computacionais para resolvê-lo: regras, aprendizado de máquina supervisionado e aprendizado de máquina não supervisionado. A tokenização é um dos processos importantes no pré-processamento de textos, que consiste em separar o texto em unidades linguísticas mínimas, chamadas de tokens. Essa tarefa pode ser feita utilizando regras ou abordagens de aprendizado de máquina. Além disso, o conceito de subpalavra também é utilizado, principalmente em modelos de linguagem baseados em redes neurais, para reduzir o tamanho do vocabulário de trabalho.\n",
            "O texto descreve a necessidade de representar textos em linguagem natural de forma vetorial, utilizando um conjunto de treinamento com um vocabulário finito. Para representar palavras em português, que possui mais de 800.000 palavras, podem ser utilizadas subpalavras como prefixos e sufixos. Existem três algoritmos frequentemente utilizados para escolher as subpalavras que compõem o vocabulário de trabalho: BPE, Word-Piece e Unigram. A normalização é a tarefa de converter as palavras para uma forma padrão, como a conversão de abreviações, caracteres minúsculos, lematização e radicalização. A escolha da normalização depende do propósito de processamento textual. A conversão de abreviações é baseada em listas predefinidas, mas cuidados devem ser tomados para evitar substituições incorretas. A conversão para caracteres minúsculos pode ser facilmente implementada, mas é necessário levar em conta a semântica associada ao uso de maiúsculas e minúsculas.\n",
            "O texto discute algumas tarefas de processamento de linguagem natural, como a conversão para caracteres minúsculos, a lematização e a radicalização. Também aborda a etiquetagem morfossintática, que envolve a atribuição de etiquetas gramaticais a cada palavra em um texto. São apresentadas diferentes abordagens para essa tarefa, como as baseadas em regras, em redes neurais e as abordagens estocásticas e híbridas.\n",
            "Nesta seção, é explicado o processo de anotação de atributos morfológicos, que envolve a marcação de informações gramaticais e morfológicas de palavras em um texto. Esses atributos incluem número, gênero, modo, tempo, pessoa e outras características. A anotação é importante para que algoritmos de Processamento de Linguagem Natural (PLN) e Aprendizado de Máquina (AM) possam entender e processar corretamente a estrutura e as relações linguísticas de um texto. A quantidade e os nomes das etiquetas podem variar entre diferentes etiquetadores morfológicos, o que constitui um desafio para o PLN. Existem abordagens computacionais para a anotação automática desses atributos morfológicos. Além disso, são mencionadas outras tarefas de processamento de texto, como análise sintática automática, segmentação de constituintes sintáticos e extração de entidades nomeadas.\n",
            "O PortiLexicon-UD é um recurso léxico para o português que elenca palavras e suas anotações morfossintáticas. Ele utiliza o padrão Universal Dependencies (UD) com etiquetas PoS, lema e etiquetas de atributos morfológicos. O léxico possui 1.226.339 entradas e mapeia palavras em 12 classes gramaticais da UD. As palavras do léxico podem conter informações morfológicas como gênero, número, forma verbal e tipo numérico. As classes ADP, ADV, CCONJ, INTJ e SCONJ geralmente têm pouca informação morfológica, enquanto as classes ADJ, NOUN, NUM, PRON e DET têm maior variação de atributos morfológicos. As classes AUX e VERB possuem uma variação de atributos morfológicos padronizada, relacionada à conjugação dos tempos verbais. O léxico contém principalmente palavras não ambíguas, mas também algumas palavras ambíguas, como 'que'.\n",
            "A palavra 'fora' possui o maior número de entradas no PortiLexicon-UD, sendo anotada com 5 etiquetas PoS distintas e possuindo onze entradas no léxico. O PortiLexicon-UD não representa palavras compostas, ênclises, mesóclises ou palavras contraídas. O léxico pode fornecer suporte à atribuição de etiquetas morfológicas e análise de características morfológicas. A menor unidade significativa da língua é o morfema, estudado pela Morfologia, enquanto em PLN, a menor unidade de processamento automático é a palavra, estudada pela Morfossintaxe. Existem recursos lexicais disponíveis para o português, como thesaurus, redes semânticas, léxicos, dicionários e ontologias. O processo de tokenização pode ser baseado na presença de espaços em branco delimitadores das palavras. Palavras desconhecidas podem impactar o processamento de modelos computacionais. A tarefa de radicalização em PLN pode considerar o radical ou a raiz.\n",
            "Here is a list of various tools and resources for natural language processing:\n",
            "\n",
            "1. LX-Tagger and LX-Suite from PortulanCLARIN: Tools for part-of-speech tagging and lemmatization.\n",
            "2. NLTK (Natural Language Toolkit): A Python library for NLP tasks.\n",
            "3. SpaCy: An industrial-strength NLP library with various features.\n",
            "4. RSLP Stemmer and Snowball Stemmer: Stemming algorithms for the Portuguese language.\n",
            "5. Sentence segmentation resources: NLTK and SpaCy provide tools for sentence segmentation.\n",
            "6. Enelvo: A tool for Portuguese language normalization and expansion.\n",
            "7. Freeling: A suite of language analyzers for several languages including Portuguese.\n",
            "8. OpenNLP: A machine learning-based toolkit for NLP tasks.\n",
            "9. NLPyPort: A library for Portuguese text processing.\n",
            "10. PolyGlot: A multilingual NLP library with support for Portuguese.\n",
            "11. StanfordNLP: A suite of NLP tools developed by Stanford University.\n",
            "12. TreeTagger: A tool for part-of-speech tagging developed by the University of Munich.\n",
            "13. Linguakit: A library for various NLP tasks in Portuguese.\n",
            "14. Universal Dependencies: A framework for annotating and parsing natural language corpora.\n",
            "15. Hugging Face Transformers: A library for natural language understanding using pre-trained models.\n",
            "16. PortTagger from Hugging Face: A tool for part-of-speech tagging in Portuguese.\n",
            "17. UDPipe: A trainable pipeline for various NLP tasks including part-of-speech tagging.\n",
            "18. MorphoDiTa: A morphological dictionary and tagger for the Czech language.\n",
            "19. StanfordNLP: A suite of NLP tools developed by Stanford University, including part-of-speech tagging.\n",
            "20. Mac-Morpho Corpus: A corpus of Brazilian Portuguese text.\n",
            "21. Floresta Sintá(c)tica: A syntactic treebank for Portuguese.\n",
            "22. POETISA: A semantic role labeling framework for Portuguese.\n",
            "23. MorphoBr: A morphological analyzer and generator for Brazilian Portuguese.\n",
            "24. NILC: A research group at the University of São Paulo that develops various NLP resources and tools.\n",
            "25. Linguateca: A website that provides various resources and tools for Portuguese NLP.\n",
            "26. Universal Dependencies Project: A project that aims to develop cross-linguistically consistent treebank annotation for many languages.\n",
            "27. PortiLexicon-UD: A lexicon for Portuguese based on Universal Dependencies.\n",
            "28. Tupla: A term used in computer science to refer to a sequence of two or more items.\n",
            "29. TEP2: A platform for collaborative development of Portuguese language resources.\n",
            "30. WordNet-BR: A Brazilian Portuguese version of WordNet.\n",
            "31. VERBNET-BR and PropBank-BR: Projects that adapt the VerbNet and PropBank resources for Brazilian Portuguese.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Capítulo 22 - PLN no Direito"
      ],
      "metadata": {
        "id": "2ATD2TqkrGTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Número de tokens do texto original**"
      ],
      "metadata": {
        "id": "GoW4aSxYrK0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_chapter_22 = data[\"Capítulo 22: PLN no Direito\"]\n",
        "no_tokens_original_text = 0\n",
        "\n",
        "for text_chunk in result_chapter_22:\n",
        "  tokens = nltk.word_tokenize(text_chunk[\"text\"])\n",
        "  no_tokens_original_text += len(tokens)\n",
        "\n",
        "print(no_tokens_original_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpngIqLdrJPO",
        "outputId": "9e58b136-c16d-46bb-b338-63e0b2d39cd6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Número de tokens do texto resumido**"
      ],
      "metadata": {
        "id": "GPwNRqZWrVgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "no_tokens_summarized_text = 0\n",
        "\n",
        "for text_chunk in result_chapter_22:\n",
        "  tokens = nltk.word_tokenize(text_chunk[\"summarized_text\"])\n",
        "  no_tokens_summarized_text += len(tokens)\n",
        "\n",
        "print(no_tokens_summarized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cn3pgWwmrTds",
        "outputId": "7fafa8a7-2a7b-473c-84b0-2305b3ae6d06"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Texto resumido**"
      ],
      "metadata": {
        "id": "TWDcmZ2xraHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarized_text = \"\"\n",
        "\n",
        "for text_chunk in result_chapter_22:\n",
        "  summarized_chunk = text_chunk[\"summarized_text\"]\n",
        "  summarized_text += summarized_chunk + \"\\n\"\n",
        "\n",
        "print(summarized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6ecQmDMrYIR",
        "outputId": "e92a8f20-c32c-43bf-9573-bf13b070f5c1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neste capítulo, são abordadas diversas perspectivas e desafios relacionados ao trabalho computacional com textos jurídicos e legais. O objetivo é apresentar algumas abordagens e estudos no contexto do Direito Brasileiro. São discutidos aspectos sócio-históricos do Direito no Brasil, o reconhecimento e exploração do vocabulário jurídico, a análise de sentimentos em textos jurídicos e o uso de técnicas de Recuperação da Informação. O capítulo também menciona a importância de estudos históricos sobre a linguagem jurídica brasileira e a normalização de textos antigos para o processamento computacional. A análise de conteúdos em sentenças judiciais via Análise de Sentimentos é destacada como uma técnica útil para identificar padrões de decisões judiciais. A criação de métodos que possam filtrar e identificar causas favoráveis em um dado tema pode ajudar os profissionais do Direito a encontrar a informação que precisam.\n",
            "O direito se manifesta através da língua, sendo as palavras e enunciados que conferem e confirmam sua existência peculiar como uma prática social e área de conhecimento. Existe uma relação intensa entre o direito e a língua, com diferentes tipos de linguagem jurídica dependendo do contexto e propósito. No Brasil, o direito é regido pelo sistema da civil law, onde a lei tem preponderância sobre a jurisprudência. A jurisprudência, entretanto, possui influência nas decisões judiciais. Quando trabalhando com textos jurídicos, é importante compreender os elementos e valores diferenciados, para evitar misturar conceitos. A terminologia jurídica estuda os diferentes fenômenos linguísticos na comunicação técnico-científica do direito. Uma palavra se torna um termo técnico-científico em determinados contextos, conferindo a ela um significado especial, chamado de conceito.\n",
            "A palavra 'comum' assume diferentes significados no contexto do Estatuto da Criança e do Adolescente (ECA), que define criança como pessoa até 12 anos incompletos e adolescente como pessoa entre 12 e 18 anos. Isso mostra a importância de considerar o contexto jurídico ao estudar o tema das crianças em leis e documentos em português. Além disso, o ECA está subordinado à Constituição do Brasil e existem leis locais que podem variar o tratamento das crianças em diferentes estados. Ao realizar um estudo de textos relacionados aos direitos das crianças, é importante identificar os termos, seus conceitos e termos/conceitos relacionados. Vocabulários controlados e normas técnicas podem ajudar a lidar com a variação terminológica. Isso é semelhante à padronização de terminologias na área da Saúde, como a Classificação Internacional de Doenças (CID).\n",
            "O Sarampo é uma doença no Brasil geralmente associada a crianças. Os tribunais como o Supremo Tribunal Federal e Superior Tribunal de Justiça criaram um site denominado 'Tesauro' para padronizar a informação. Tesauros são listas de assuntos e terminologias de uma área de conhecimento. Essas listagens são usadas na indexação e catalogação de documentos. O trabalho de um Terminólogo (RT) envolve a mediação terminológica e pode incluir a análise de documentos jurídicos e legislação. Os tesauros dos tribunais oferecem informações sobre termos relacionados ao Direito Ambiental. Há também dicionários especializados nessa área, como o Dicionário de Direito Ambiental do Grupo Termisul da UFRGS.\n",
            "Os processos judiciais podem ser julgados com base em termos do legislativo, mas o tesauro do Supremo Tribunal Federal tem um nível de detalhamento maior. O tesauro elaborado por alguns tribunais reduz esforços na aplicação do Processamento de Linguagem Natural (PLN) ao Direito, pois já associa os termos mais frequentes nas decisões judiciais. A análise de sentimento em textos jurídicos envolve a determinação do tom emocional ou opinativo presente nos documentos legais. Para isso, são seguidos passos como coleta de dados, pré-processamento do texto, rotulação de dados, escolha da técnica de análise de sentimento, execução da técnica e avaliação e validação. A coleta de dados pode ser feita em diários de justiça, APIs públicas como o DataJud do Conselho Nacional de Justiça e sistemas de busca dos tribunais. O pré-processamento do texto envolve a tokenização, remoção de pontuações, conversão para minúsculas e remoção de stop words. Após o pré-processamento, pode-se aperfeiçoar a tarefa incluindo novas stop words para limpar ainda mais o texto.\n",
            "Neste texto, é discutido o processo de análise de sentimento em decisões judiciais. São apresentadas três fases do processo: remoção de palavras irrelevantes, rotulamento de dados e escolha da técnica de análise de sentimento. Na fase de escolha da técnica, são mencionadas duas abordagens: baseada em regras e baseada em aprendizado de máquina. No exemplo dado, é utilizada a abordagem baseada em regras, com o uso de um dicionário prévio de palavras positivas e negativas. Após a execução da técnica, é calculado um sentimento geral, que pode ser positivo, negativo ou neutro. O texto ressalta que o processo de análise de sentimento em textos jurídicos é complexo devido às nuances e peculiaridades discursivas, e são necessários ajustes e validações em cada fase do processo.\n",
            "O uso de processamento de linguagem natural no campo do Direito enfrenta desafios na coleta de dados, já que os tribunais não possuem repositórios com informações estruturadas e públicas. Algoritmos de web crawling e web scraping podem contornar esse problema, mas alguns tribunais utilizam captchas para impedir acesso automatizado. A limpeza e seleção das informações também são custosas. Mesmo assim, existem iniciativas como o DataJud, do Conselho Nacional de Justiça, que centraliza e fornece informações estruturadas para análise. O capítulo destaca a importância de considerar as características e elementos de diferentes tipos de documentos jurídicos. Também menciona um estudo sobre a complexidade das sentenças dos Juizados Especiais Cíveis e oferece acesso a um corpus de sentenças analisadas. O capítulo conclui ressaltando que o Direito é um campo fértil para o trabalho de análise linguístico-textual, beneficiando profissionais e a sociedade como um todo.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vYwqPiNwrdQj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}